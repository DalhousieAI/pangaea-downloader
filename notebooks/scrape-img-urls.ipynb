{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download datasets with images hosted on website\n",
    "Most of the benthic habitat datasets have image urls along with other metadata stored in tabular format (dataframe). However some datasets host the images on the website. So they have to be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pangaeapy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.compat import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the URLs/DOIs of some datasets without image urls in tabular format. We will scrape one of them to test the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Christiansen, B (2006)\n",
    "dois = [\n",
    "    \"https://doi.org/10.1594/PANGAEA.371062\",\n",
    "    \"https://doi.org/10.1594/PANGAEA.371063\",\n",
    "    \"https://doi.org/10.1594/PANGAEA.371064\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ids = [int(dsid.split(\".\")[-1]) for dsid in dois]\n",
    "ds_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Request dataset url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one of the dois\n",
    "idx = 0\n",
    "doi = dois[idx]\n",
    "ds_id = ds_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pangaeapy.PanDataSet(ds_id)\n",
    "print(\"Dataset title:\", dataset.title)\n",
    "print(\"Requesting:\", doi)\n",
    "resp = requests.get(doi)\n",
    "soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "# Get download link to photos page\n",
    "download_link = soup.find(\"div\", attrs={\"class\": \"text-block top-border\"}).a[\"href\"]\n",
    "print(\"URL to photos page:\", download_link)\n",
    "# Get to photos page (page 1)\n",
    "resp = requests.get(download_link)\n",
    "photos_page = BeautifulSoup(resp.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(page_soup):\n",
    "    coordinates = page_soup.find(\"div\", attrs={\"class\": \"hanging geo\"})\n",
    "    lat = float(coordinates.find(\"span\", attrs={\"class\": \"latitude\"}).text)\n",
    "    long = float(coordinates.find(\"span\", attrs={\"class\": \"longitude\"}).text)\n",
    "    return lat, long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, long = get_metadata(soup)\n",
    "print(f\"Lat: {lat}, Long: {long}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get pagination info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagination(page_soup, src_url=\"https://www.pangaea.de/helpers/Benthos.php\"):\n",
    "    \"\"\"\n",
    "    Take a BeautifulSoup object and return a dictionary with page numbers and URLs.\n",
    "    \"\"\"\n",
    "    # <p> tag containing pagination info\n",
    "    pagination = page_soup.find(\"p\", attrs={\"class\": \"navigation\"})\n",
    "    # Page numbers (strs)\n",
    "    page_nums = [i.strip() for i in pagination.text.split(\"|\")][2:-1]\n",
    "    # List of page URLs\n",
    "    page_urls = [urljoin(src_url, a[\"href\"]) for a in pagination.find_all(\"a\")][:-1]\n",
    "    # Page number : Page URL\n",
    "    page_dict = {k: v for k, v in zip(page_nums, page_urls)}\n",
    "    return page_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagination = get_pagination(photos_page)\n",
    "for k in pagination:\n",
    "    print(k, \":\", pagination[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get image URLs from page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_urls(page_soup, verbose=False):\n",
    "    \"\"\"\n",
    "    Take a BeautifulSoup object and return list of image urls.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "\n",
    "    table = page_soup.find(\"table\", class_=\"pictable\")\n",
    "    photos = table.find_all(\"td\")\n",
    "    if verbose:\n",
    "        print(\"[INFO] Number of photos on page:\", len(photos))\n",
    "\n",
    "    # urls = [\"https:\"+td.a['href'] for td in photos]\n",
    "    for td in photos:\n",
    "        try:\n",
    "            url = \"https:\" + td.a[\"href\"]\n",
    "        except TypeError:\n",
    "            # The last <td> of the last page is sometimes empty\n",
    "            # No photos, just a blank <td> tag\n",
    "            print(\"[WARNING] Empty <td> tag encountered!\")\n",
    "        urls.append(url)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_urls = get_image_urls(photos_page, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url in img_urls:\n",
    "#     print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scrape all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_dataset(page_soup):\n",
    "    pagination = get_pagination(page_soup)\n",
    "    # Scrape current page\n",
    "    print(\"[INFO] Processing Page 1...\")\n",
    "    img_urls = get_image_urls(page_soup, verbose=True)\n",
    "    # Scraper subsequent pages\n",
    "    for n in pagination:\n",
    "        print(f\"[INFO] Processing Page {n}...\")\n",
    "        url = pagination[n]\n",
    "        resp = requests.get(url)\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        urls = get_image_urls(soup, verbose=True)\n",
    "        img_urls.extend(urls)\n",
    "    return img_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scrape_dataset(photos_page)\n",
    "print(f\"[INFO] Total {len(data)} images scraped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data\n",
    "df = pd.DataFrame(data, columns=[\"url\"])\n",
    "df[\"image\"] = df[\"url\"].apply(lambda url: url.split(\"/\")[-1])\n",
    "df[\"long\"] = long\n",
    "df[\"lat\"] = lat\n",
    "df[\"site\"] = dataset.events[0].label\n",
    "df[\"campagin\"] = dataset.events[0].campaign\n",
    "df[\"dataset\"] = dataset.title\n",
    "\n",
    "# Rearranging columns\n",
    "df = df[df.columns[::-1]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure output directory exists\n",
    "out_dir = \"../outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "# Save to file\n",
    "file = f\"{out_dir}/[scraped]{ds_id}.csv\"\n",
    "df.to_csv(file, index=False)\n",
    "print(f\"Saved at: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(pangaeapy.PanDataSet(doi).citation, end=\"\\n\\n\") for doi in dois];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (ws)",
   "language": "python",
   "name": "ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
