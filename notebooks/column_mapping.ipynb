{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab3f23-0a3d-4df4-abb6-20f61fe0cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pangaea_downloader.tools import checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35131f3-6be1-4db6-bd98-1517d62cf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"../query-outputs2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f2898-25da-4cea-99fa-3aa74a4f4e3e",
   "metadata": {},
   "source": [
    "## Load datasets and check distribution of column frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e38b1-f7e7-4e9e-a55f-7a9ac6adf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url_column(df):\n",
    "\n",
    "    clean_cols = [\n",
    "        col.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\").replace(\".\", \"\")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    # Ordered list of priorities\n",
    "    # Exclude url meta/ref/source which are not links to images\n",
    "    candidates = [\n",
    "        \"urlimage\",\n",
    "        \"urlraw\",\n",
    "        \"urlfile\",\n",
    "        \"url\",\n",
    "        \"urlgraphic\",\n",
    "        \"urlthumb\",\n",
    "        \"urlthumbnail\",\n",
    "        \"image\",\n",
    "        \"imagery\",\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if candidate not in clean_cols:\n",
    "            continue\n",
    "        col = df.columns[clean_cols.index(candidate)]\n",
    "        if any(df[col].apply(checker.is_url)):\n",
    "            return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e6ccb-2634-48ff-928b-54c73579c36b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_count = defaultdict(lambda: 0)\n",
    "column_examples = defaultdict(lambda: [])\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "for fname in tqdm(os.listdir(dirname)):\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    n_total += 1\n",
    "    if not checker.has_url_col(df):\n",
    "        continue\n",
    "    url_col = find_url_column(df)\n",
    "    if not url_col:\n",
    "        print(f\"No url column for {fname} with columns\\n{df.columns}\")\n",
    "        files_without_url.append(fname)\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    for col in df.columns:\n",
    "        col = col.lower().strip()\n",
    "        column_count[col] += 1\n",
    "        column_examples[col].append(fname)\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b763999-c70c-45be-91f5-806a7fef5f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) valid datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b478a-bd3d-417f-8e88-f49ea585c812",
   "metadata": {},
   "source": [
    "### Examining columns to find out what their contents are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df384e8f-569d-4ba7-9ac5-94215cf73db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"resolution\"][0]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb89dc0-4eb3-437a-8db9-95bc4778c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "n_datasets = 0\n",
    "for fname in column_examples[\"seagr cov\"]:\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    url_column = find_url_column(df)\n",
    "    if url_column:\n",
    "        urls.append(df[url_column])\n",
    "        n_datasets += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fb907-9719-4250-9fdd-1b274641a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.concat(urls).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775e137-d8d2-4b11-959e-300c053f3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf521eb-f434-47b2-bc07-db240dc34bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(column_examples[\"seagr cov\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65082d0e-9d20-451c-b9cf-3ed816067a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(column_examples[\"seagr cov\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe71a6-83f8-47c2-b290-ce879f1e3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_columns = [k for k in column_examples.keys() if k.endswith(\" cov\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268bd50-1560-4b55-8092-31c02f424dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coverage_datasets = set()\n",
    "for k in coverage_columns:\n",
    "    if k == \"ice cov\":\n",
    "        continue\n",
    "    print()\n",
    "    print(k)\n",
    "    print(len(coverage_datasets))\n",
    "    coverage_datasets.update(column_examples[k])\n",
    "    print(len(coverage_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee86b7-c394-4ead-a05d-d867fb923a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coverage_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a0dcf-7397-4165-8fe4-c087470a1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "n_datasets = 0\n",
    "for fname in coverage_datasets:\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    url_column = find_url_column(df)\n",
    "    if url_column:\n",
    "        urls.append(df[url_column])\n",
    "        n_datasets += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a29df-7717-4c8e-a356-d975a879d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.concat(urls).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7a847-1d8a-4f43-a2c8-dcd18b4c22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"ice cov\"][1]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d57b4-5410-41ac-bfd7-952101e4415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"ph\"][0]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884eaf21-56f2-4da1-8627-45b334c29369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"elevation\"][-1]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dffa8b-0075-4a63-b547-c84d51e7bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, files_with_repeat_urls[3]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd2af0-f9b7-46dc-80ab-fcc8ab54bf45",
   "metadata": {},
   "source": [
    "## Implement dataset cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7111b-cca5-4f29-b49b-806ca744c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXONOMY_RANKS = [\n",
    "    [\"Kingdom\", \"Regnum\"],\n",
    "    [\"Phylum\", \"Division\"],\n",
    "    [\"Ordo\", \"Order\"],\n",
    "    [\"Familia\", \"Family\"],\n",
    "    [\"Genus\"],\n",
    "    [\"Species\"],\n",
    "]\n",
    "\n",
    "\n",
    "def row2taxonomy(row):\n",
    "    parts = []\n",
    "    for rank_synonyms in TAXONOMY_RANKS:\n",
    "        for col in rank_synonyms:\n",
    "            col_ = col.lower()\n",
    "            if col in row.keys() and row[col] and row[col] != \"-\":\n",
    "                parts.append(row[col])\n",
    "                break\n",
    "            elif col_ in row.keys() and row[col_] and row[col_] != \"-\":\n",
    "                parts.append(row[col_])\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return \" > \".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1931b2-0617-421f-ae7f-ed62c2b0cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_title(title):\n",
    "    \"\"\"\n",
    "    Screen dataset title.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        The title of the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Whether the dataset title is acceptable.\n",
    "    \"\"\"\n",
    "\n",
    "    title = str(title)\n",
    "\n",
    "    if title.startswith(\"Meteorological observations\"):\n",
    "        return False\n",
    "    if title.startswith(\"Sea ice conditions\"):\n",
    "        return False\n",
    "    if \"topsoil\" in title.lower():\n",
    "        return False\n",
    "    if \"core\" in title.lower():\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def add_file_extension(row):\n",
    "    \"\"\"\n",
    "    Add file extension to image filename.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : dict\n",
    "        A dict record which may have fields ``\"image\"``, ``\"File format\"``, ``\"File type\"``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fname : str\n",
    "        File name with extension included.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        \"image\" not in row.keys()\n",
    "        or not row[\"image\"]\n",
    "        or not isinstance(row[\"image\"], str)\n",
    "    ):\n",
    "        return \"\"\n",
    "\n",
    "    s = row[\"image\"]\n",
    "    ext = os.path.splitext(s)[-1]\n",
    "    if (\n",
    "        ext.lower()\n",
    "        in checker.VALID_IMG_EXTENSIONS\n",
    "        + checker.INVALID_FILE_EXTENSIONS\n",
    "        + checker.COMPRESSED_FILE_EXTENSIONS\n",
    "    ):\n",
    "        return s\n",
    "\n",
    "    for col in [\"File format\", \"File type\"]:\n",
    "        if col not in row.keys():\n",
    "            continue\n",
    "        new_ext = row[col]\n",
    "        if not new_ext or not isinstance(new_ext, str):\n",
    "            continue\n",
    "        new_ext = \".\" + new_ext.strip().lstrip(\".\")\n",
    "        if ext == new_ext:\n",
    "            break\n",
    "        s += new_ext\n",
    "        break\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def datetime2timestamp(ts):\n",
    "    \"\"\"\n",
    "    Convert a datetime string to a timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ts : str\n",
    "        Datetime string, in a format understood by ``dateutil.parser``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Timestamp; number of seconds since Unix epoch.\n",
    "    \"\"\"\n",
    "    if isinstance(ts, str):\n",
    "        return dateutil.parser.parse(ts).timestamp()\n",
    "    return ts\n",
    "\n",
    "\n",
    "def reformat_df(df, remove_duplicate_columns=True):\n",
    "    \"\"\"\n",
    "    Reformat/clean pangaea dataset.\n",
    "\n",
    "    Rename columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Original dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame or None\n",
    "        Cleaned dataset, or ``None`` if the dataset is invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    if (\n",
    "        \"dataset_title\" in df\n",
    "        and len(df) > 0\n",
    "        and df.iloc[0][\"dataset_title\"]\n",
    "        and not check_title(df.iloc[0][\"dataset_title\"])\n",
    "    ):\n",
    "        return None\n",
    "\n",
    "    # Make a copy of the dataframe so we can't overwrite the input\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove bad columns\n",
    "    df.drop(labels=[\"-\"], axis=\"columns\", inplace=True, errors=\"ignore\")\n",
    "    # Remove duplicately named columns\n",
    "    cols_to_drop = []\n",
    "    if remove_duplicate_columns:\n",
    "        for col in df.columns:\n",
    "            if len(col) < 2:\n",
    "                continue\n",
    "            if (\n",
    "                (col[-2] in \" _\")\n",
    "                and (col[-1] in \"123456789\")\n",
    "                and (col[:-2] in df.columns)\n",
    "            ):\n",
    "                cols_to_drop.append(col)\n",
    "        df.drop(labels=cols_to_drop, axis=\"columns\", inplace=True)\n",
    "\n",
    "    # Find the correct URL column, and drop other columns containing \"url\"\n",
    "    cols_to_drop = []\n",
    "    mapping = {}\n",
    "    col_url = find_url_column(df)\n",
    "    mapping[col_url] = \"url\"\n",
    "    for col in df.columns:\n",
    "        if col != col_url and \"url\" in col.lower():\n",
    "            cols_to_drop.append(col)\n",
    "\n",
    "    # Search for matches to canonical columns.\n",
    "    # Each entry in desired_columns is a key, value pair where the key\n",
    "    # is the output column name, and the value is a list of search names\n",
    "    # in order of priority. The first match will be kept and others discarded.\n",
    "    desired_columns = {\n",
    "        \"dataset\": [\"dataset\", \"Campaign\", \"campaign\"],\n",
    "        \"site\": [\"Event\", \"event\", \"Site\", \"site\", \"deployment\"],\n",
    "        \"image\": [\"image\", \"filename\"],\n",
    "        \"timestamp\": [\"Timestamp\"],\n",
    "        \"latitude\": [\"Latitude\", \"latitude\", \"lat\", \"latitude+\"],\n",
    "        \"longitude\": [\"Longitude\", \"longitude\", \"lon\", \"long\", \"longitude+\"],\n",
    "        \"x_pos\": [],\n",
    "        \"y_pos\": [],\n",
    "        \"altitude\": [\"altitude\", \"height\"],\n",
    "        \"depth\": [\n",
    "            \"depthwater\",\n",
    "            \"bathydepth\",\n",
    "            \"bathymetry\",\n",
    "            \"bathy\",\n",
    "            \"depth\",\n",
    "            \"elevation\",\n",
    "        ],\n",
    "        \"backscatter\": [],\n",
    "        \"temperature\": [\"temperature\", \"temp\"],\n",
    "        \"salinity\": [\"salinity\", \"sal\"],\n",
    "        \"chlorophyll\": [],\n",
    "        \"acidity\": [\"pH\"],\n",
    "        \"doi\": [\"DOI\", \"doi\"],\n",
    "    }\n",
    "    # Remove non-alphanumeric padding characters, including spaces, from actual column names\n",
    "    clean_cols = [\n",
    "        col.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\").replace(\".\", \"\")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    # Map to lower case\n",
    "    lower_cols = [col.lower() for col in clean_cols]\n",
    "\n",
    "    # Search for matching column names\n",
    "    for canon, searches in desired_columns.items():\n",
    "        found = False\n",
    "\n",
    "        # Check for case-sensitive match\n",
    "        for search in searches:\n",
    "            if search not in clean_cols:\n",
    "                continue\n",
    "            col = df.columns[clean_cols.index(search)]\n",
    "            if not found:\n",
    "                found = True\n",
    "                mapping[col] = canon\n",
    "                if col != canon and canon in df.columns:\n",
    "                    cols_to_drop.append(canon)\n",
    "            else:\n",
    "                cols_to_drop.append(col)\n",
    "\n",
    "        # Check for case-insensitive match\n",
    "        for search in searches:\n",
    "            if search.lower() not in lower_cols:\n",
    "                continue\n",
    "            col = df.columns[lower_cols.index(search.lower())]\n",
    "            if not found:\n",
    "                found = True\n",
    "                mapping[col] = canon\n",
    "                if col != canon and canon in df.columns:\n",
    "                    cols_to_drop.append(canon)\n",
    "            elif col not in mapping and col not in cols_to_drop:\n",
    "                cols_to_drop.append(col)\n",
    "\n",
    "    # Remove superfluous columns\n",
    "    df.drop(labels=cols_to_drop, axis=\"columns\", inplace=True)\n",
    "    # Rename columns to canonical names\n",
    "    df.rename(columns=mapping, inplace=True, errors=\"raise\")\n",
    "\n",
    "    # Add file extension to image\n",
    "    df[\"image\"] = df.apply(add_file_extension, axis=1)\n",
    "    if \"timestamp\" not in df.columns and \"Date/Time\" in df.columns:\n",
    "        df[\"timestamp\"] = df[\"Date/Time\"].apply(datetime2timestamp)\n",
    "\n",
    "    if any([c in clean_cols for c in [\"Kingdom\", \"Phylum\", \"Genus\"]]):\n",
    "        df[\"taxonomy\"] = df.apply(row2taxonomy, axis=1)\n",
    "        df.drop(\n",
    "            labels=[x for syn in TAXONOMY_RANKS for x in syn],\n",
    "            axis=\"columns\",\n",
    "            inplace=True,\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "\n",
    "    cols_to_drop = [\n",
    "        \"File format\",\n",
    "        \"File type\",\n",
    "        \"File size\",\n",
    "        \"Date/Time\",\n",
    "        \"Date/time end\",\n",
    "    ]\n",
    "    df.drop(labels=cols_to_drop, axis=\"columns\", inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21314a94-a03c-4317-87fb-0578e3f04381",
   "metadata": {},
   "source": [
    "## Load data with dataset cleaning functions applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed7c35-614c-40b8-a192-e3ef24b984cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_count = defaultdict(lambda: 0)\n",
    "column_examples = defaultdict(lambda: [])\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "dfs = []\n",
    "\n",
    "for fname in tqdm(os.listdir(dirname)):\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    n_total += 1\n",
    "    if not checker.has_url_col(df):\n",
    "        continue\n",
    "    # print(df.columns)\n",
    "    url_col = find_url_column(df)\n",
    "    if not url_col:\n",
    "        # print(f\"No url column for {fname} with columns\\n{df.columns}\")\n",
    "        files_without_url.append(fname)\n",
    "        continue\n",
    "    df = reformat_df(df)\n",
    "    if df is None:\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    dfs.append(df)\n",
    "    for col in df.columns:\n",
    "        column_count[col] += 1\n",
    "        column_examples[col].append(fname)\n",
    "    # print(df.columns)\n",
    "    url_col = \"url\"\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e5487-685f-49ec-aac3-2af68b942a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) valid datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315634da-ccc1-45f5-9eff-c298c518e765",
   "metadata": {},
   "source": [
    "### Merge datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666b6ba-7513-41fc-b504-76b40d6e9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cols = {\n",
    "    \"dataset\",\n",
    "    \"site\",\n",
    "    \"url\",\n",
    "    \"image\",\n",
    "    \"timestamp\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"x_pos\",\n",
    "    \"y_pos\",\n",
    "    \"altitude\",\n",
    "    \"depth\",\n",
    "    \"backscatter\",\n",
    "    \"temperature\",\n",
    "    \"salinity\",\n",
    "    \"chlorophyll\",\n",
    "    \"acidity\",\n",
    "}\n",
    "\n",
    "df_all = pd.concat([df[df.columns.intersection(select_cols)] for df in dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa83a9-05fd-4161-be11-3f6369f8a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eddfd9-5541-4681-8da5-8ec4b8950eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all records\n",
    "df_all.to_csv(\n",
    "    f\"../pangaea_{datetime.datetime.today().strftime('%Y-%m-%d')}.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082806f5-8aeb-4b97-8e39-c5749b3b7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only valid URLs\n",
    "df_all = df_all[df_all[\"url\"].apply(checker.is_url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4a895-2086-49c4-8c67-1b1930b76c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ffef8-ff55-48f6-b588-ffbdf036145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:4])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f7f88-d2a5-43b5-9190-5af4a6c058e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8dd81e-1126-4bb8-a2bd-2c66c6785da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = unique_url_bases[0]\n",
    "df_all[df_all[\"url\"].str.startswith(url_base)].iloc[[0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8106df-d060-43ec-b91e-3a23fa7ee1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c7b28-64fb-4950-8082-94f884f329ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows have lat & lon\n",
    "sum(~df_all[\"latitude\"].isna() & ~df_all[\"longitude\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a511931-45ab-46a6-a37c-337ee9324fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837620f-29ba-421b-93ae-1ff1dda78b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate URLs\n",
    "df_all = df_all.drop_duplicates(subset=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98d486-cf02-4797-98e4-dafaed33a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37dcd1b-345c-4a9a-8c0a-916913f296c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows have lat & lon\n",
    "sum(~df_all[\"latitude\"].isna() & ~df_all[\"longitude\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456fb65-4762-4d27-9585-abf6d23cead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693c9c0-96d4-4006-b3cc-41bfb959a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_image = df_all[\"url\"].apply(\n",
    "    lambda x: checker.has_img_extension(x.rstrip(\"/\"))\n",
    ") | df_all[\"image\"].apply(lambda x: checker.has_img_extension(x.rstrip(\"/\")))\n",
    "df_all = df_all[is_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb88409-50aa-463c-824f-c74d76180271",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d8862-607e-48f7-b8ff-dfa4fa54be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(~df_all[\"latitude\"].isna() & ~df_all[\"longitude\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807fed2-52b0-4f7c-97c9-9833679fd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f48d22-7fdd-4d45-80e0-7413d2568df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:4])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46de5eb-5a71-4ce2-9f12-139a482c8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a0418-aa84-41a9-94b9-2780d7eba123",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5b1f9-9148-4858-a1ed-d5a5d225f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subdomain(url):\n",
    "    blacklist = [\n",
    "        \"http://epic.awi.de/\",\n",
    "        \"https://epic.awi.de/\",\n",
    "        \"http://hdl.handle.net/10013/\",\n",
    "        \"http://library.ucsd.edu/dc/object/\",\n",
    "        \"https://hs.pangaea.de/Maps/\",\n",
    "        \"https://hs.pangaea.de/Movies/\",\n",
    "        \"https://hs.pangaea.de/Projects/\",\n",
    "        \"https://hs.pangaea.de/bathy/\",\n",
    "        \"https://hs.pangaea.de/fishsounder/\",\n",
    "        \"https://hs.pangaea.de/mag/\",\n",
    "        \"https://hs.pangaea.de/model/\",\n",
    "        \"https://hs.pangaea.de/nav/\",\n",
    "        \"https://hs.pangaea.de/palaoa/\",\n",
    "        \"https://hs.pangaea.de/para/\",\n",
    "        \"https://hs.pangaea.de/reflec/\",\n",
    "        \"https://hs.pangaea.de/sat/\",\n",
    "        \"https://prr.osu.edu/collection/object/\",\n",
    "        \"https://store.pangaea.de/Projects/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/Publications/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/software/\",\n",
    "        \"https://www.ngdc.noaa.gov/geosamples/\",\n",
    "    ]\n",
    "    for entry in blacklist:\n",
    "        if url.startswith(entry):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d54c9-6f69-4229-b5ec-ad62d4876dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad URLs based on their subdomain\n",
    "df_all = df_all[df_all[\"url\"].apply(check_subdomain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd747c-1bdb-43eb-a5bd-af83bfb7494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797d5fb-929a-48f1-a55c-985bfdcbc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:5])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15212b59-e4c5-4d23-ba2c-7b9cc597400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6b436-d7ee-4748-b609-869bff5bd7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce63a9-4e1b-44a5-b95f-5d88298ce878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d14f94-f85e-4a74-b589-1e99ed53facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.search(\n",
    "    \"(?<![A-Za-z])map(?![A-Za-z])\",\n",
    "    \"https://hs.pangaea.de/Images/ROV/M/M114/GeoB19346-1/data_publish/config/divemap/Dive360_map.jpg\",\n",
    "):\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b03137-b9f3-4e4c-b46d-1a2c62430112",
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.search(\n",
    "    \"(?<![A-Za-z])map(?![A-Za-z])\",\n",
    "    \"https://hs.pangaea.de/Images/ROV/M/M114/GeoB19346-1/data_publish/config/divemap/Dive360mapy.jpg\",\n",
    "):\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd73bc0-7f1f-4aec-bca0-b4ee872a9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subdomain(url):\n",
    "    blacklist = [\n",
    "        \"http://epic.awi.de/\",\n",
    "        \"https://epic.awi.de/\",\n",
    "        \"http://hdl.handle.net/10013/\",\n",
    "        \"http://library.ucsd.edu/dc/object/\",\n",
    "        \"https://app.geosamples.org/uploads/UHM\",\n",
    "        \"https://hs.pangaea.de/Images/Linescan\",\n",
    "        \"https://hs.pangaea.de/Maps/\",\n",
    "        \"https://hs.pangaea.de//Maps\",\n",
    "        \"https://hs.pangaea.de/Movies/\",\n",
    "        \"https://hs.pangaea.de/Projects/\",\n",
    "        \"https://hs.pangaea.de/bathy/\",\n",
    "        \"https://hs.pangaea.de/fishsounder/\",\n",
    "        \"https://hs.pangaea.de/mag/\",\n",
    "        \"https://hs.pangaea.de/model/\",\n",
    "        \"https://hs.pangaea.de/nav/\",\n",
    "        \"https://hs.pangaea.de/palaoa/\",\n",
    "        \"https://hs.pangaea.de/pasata/\",\n",
    "        \"https://hs.pangaea.de/para/\",\n",
    "        \"https://hs.pangaea.de/polar\",\n",
    "        \"https://hs.pangaea.de/reflec/\",\n",
    "        \"https://hs.pangaea.de/sat/\",\n",
    "        \"https://prr.osu.edu/collection/object/\",\n",
    "        \"https://store.pangaea.de/Projects/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/Publications/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/software/\",\n",
    "        \"https://www.ngdc.noaa.gov/geosamples/\",\n",
    "        \"https://hs.pangaea.de/Images/Airphoto\",\n",
    "        # \"https://hs.pangaea.de/Images/Cores\",  # Some of these are okay\n",
    "        \"https://hs.pangaea.de/Images/Documentation\",\n",
    "        \"https://hs.pangaea.de/Images/Maps\",\n",
    "        \"https://hs.pangaea.de/Images/Plankton\",\n",
    "        \"https://hs.pangaea.de/Images/Satellite\",\n",
    "        \"https://hs.pangaea.de/Images/SeaIce\",\n",
    "        \"https://hs.pangaea.de/Images/Water\",\n",
    "        \"https://store.pangaea.de/Images/Airphoto\",\n",
    "    ]\n",
    "    banned_words = [\"map\", \"divemap\", \"dredge_photos\", \"dredgephotograph\"]\n",
    "    for entry in blacklist:\n",
    "        if url.startswith(entry):\n",
    "            return False\n",
    "    for word in banned_words:\n",
    "        if re.search(\"(?<![A-Za-z])\" + word + \"(?![A-Za-z])\", url.lower()):\n",
    "            return False\n",
    "    if re.search(\"(?<![a-z])core(?![a-rty])\", url.lower()) and \"sur\" not in url.lower():\n",
    "        # Images of cores must contain \"surface\", or the shorthand \"sur\"\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7fb7d-f80a-4788-bb22-102419e32be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad URLs based on their subdomain (again), and remove maps\n",
    "df_all = df_all[df_all[\"url\"].apply(check_subdomain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f68e32-c94a-46a5-b1ec-330b2fe182a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39795a9-49a1-4d54-b3fa-0532c403b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:5])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db689e-1288-405e-bfb6-89e58a91ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f6870-57fd-4884-84c9-34ef5df34e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[499])\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[4999])\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9bde3-cbda-402d-ba37-122b0507b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\n",
    "    f\"../pangaea_{datetime.datetime.today().strftime('%Y-%m-%d')}_filtered.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d32e61-1efc-488b-bd53-4d3d4d3b8ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10ff34d-71fa-461b-88b0-be5c1145032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_dfs = []\n",
    "for fname in tqdm(coverage_datasets):\n",
    "    ds_id = os.path.splitext(fname)[0]\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    if not checker.has_url_col(df):\n",
    "        continue\n",
    "    # print(df.columns)\n",
    "    url_col = find_url_column(df)\n",
    "    if not url_col:\n",
    "        print(f\"No url column for {fname} with columns\\n{df.columns}\")\n",
    "        continue\n",
    "    df[\"ds_id\"] = f\"pangaea-{ds_id}\"\n",
    "    df = reformat_df(df, remove_duplicate_columns=False)\n",
    "    if df is None or len(df) == 0:\n",
    "        continue\n",
    "    df = df[~df[\"url\"].isna()]\n",
    "    df = df[df[\"url\"].apply(check_subdomain)]\n",
    "    is_image = df[\"url\"].apply(lambda x: checker.has_img_extension(x.rstrip(\"/\"))) | df[\n",
    "        \"image\"\n",
    "    ].apply(lambda x: checker.has_img_extension(x.rstrip(\"/\")))\n",
    "    df = df[is_image]\n",
    "    if df is None or len(df) == 0:\n",
    "        continue\n",
    "    cov_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca4fa07-cc87-4480-8e0f-2060168447ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_all = pd.concat(cov_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2635644-6a17-4671-911e-9324cdcff727",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cov_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba475d94-08dd-412b-a0ff-a848327e77e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in sorted(df_cov_all.columns):\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
