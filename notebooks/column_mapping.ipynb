{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab3f23-0a3d-4df4-abb6-20f61fe0cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import dateutil.parser\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from benthicnet.utils import sanitize_filename, sanitize_filename_series\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pangaea_downloader.tools import checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35131f3-6be1-4db6-bd98-1517d62cf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"../query-outputs_2022-01-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f2898-25da-4cea-99fa-3aa74a4f4e3e",
   "metadata": {},
   "source": [
    "## Load datasets and check distribution of column frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e38b1-f7e7-4e9e-a55f-7a9ac6adf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url_column(df):\n",
    "\n",
    "    clean_cols = [\n",
    "        col.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\").replace(\".\", \"\")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    # Ordered list of priorities\n",
    "    # Exclude url meta/ref/source which are not links to images\n",
    "    candidates = [\n",
    "        \"urlimage\",\n",
    "        \"urlraw\",\n",
    "        \"urlfile\",\n",
    "        \"url\",\n",
    "        \"urlgraphic\",\n",
    "        \"urlthumb\",\n",
    "        \"urlthumbnail\",\n",
    "        \"image\",\n",
    "        \"imagery\",\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if candidate not in clean_cols:\n",
    "            continue\n",
    "        col = df.columns[clean_cols.index(candidate)]\n",
    "        if any(df[col].apply(checker.is_url)):\n",
    "            return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e6ccb-2634-48ff-928b-54c73579c36b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_count = defaultdict(lambda: 0)\n",
    "column_examples = defaultdict(lambda: [])\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "verbose = False\n",
    "\n",
    "for fname in tqdm(os.listdir(dirname)):\n",
    "    ds_id = os.path.splitext(fname)[0]\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    n_total += 1\n",
    "\n",
    "    if not checker.has_url_col(df):\n",
    "        continue\n",
    "\n",
    "    url_col = find_url_column(df)\n",
    "\n",
    "    if not url_col:\n",
    "        if verbose:\n",
    "            print(f\"No url column for {fname} with columns\\n{df.columns}\")\n",
    "        files_without_url.append(fname)\n",
    "        continue\n",
    "\n",
    "    n_valid += 1\n",
    "    for col in df.columns:\n",
    "        col = col.lower().strip()\n",
    "        column_count[col] += 1\n",
    "        column_examples[col].append(fname)\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b763999-c70c-45be-91f5-806a7fef5f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b478a-bd3d-417f-8e88-f49ea585c812",
   "metadata": {},
   "source": [
    "### Examining columns to find out what their contents are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df384e8f-569d-4ba7-9ac5-94215cf73db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"resolution\"][0]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb89dc0-4eb3-437a-8db9-95bc4778c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "n_datasets = 0\n",
    "for fname in column_examples[\"seagr cov\"]:\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    url_column = find_url_column(df)\n",
    "    if url_column:\n",
    "        urls.append(df[url_column])\n",
    "        n_datasets += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fb907-9719-4250-9fdd-1b274641a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.concat(urls).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775e137-d8d2-4b11-959e-300c053f3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705a972-2212-47fb-862c-d27cfa8e9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up largest dataset, containing long cruises\n",
    "df = pd.read_csv(os.path.join(dirname, \"882349.csv\"))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354140a-0c95-42b3-8863-8ceadea95f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(-df[\"Depth water\"], label=\"Depth water\")\n",
    "plt.plot(df[\"Elevation\"], label=\"Elevation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df[\"Depth water\"] + df[\"Elevation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf521eb-f434-47b2-bc07-db240dc34bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(column_examples[\"seagr cov\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65082d0e-9d20-451c-b9cf-3ed816067a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(column_examples[\"seagr cov\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe71a6-83f8-47c2-b290-ce879f1e3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_columns = [k for k in column_examples.keys() if k.endswith(\" cov\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268bd50-1560-4b55-8092-31c02f424dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coverage_datasets = set()\n",
    "for k in coverage_columns:\n",
    "    if k == \"ice cov\":\n",
    "        continue\n",
    "    print()\n",
    "    print(k)\n",
    "    print(len(coverage_datasets))\n",
    "    coverage_datasets.update(column_examples[k])\n",
    "    print(len(coverage_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee86b7-c394-4ead-a05d-d867fb923a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coverage_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a0dcf-7397-4165-8fe4-c087470a1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "n_datasets = 0\n",
    "for fname in coverage_datasets:\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    url_column = find_url_column(df)\n",
    "    if url_column:\n",
    "        urls.append(df[url_column])\n",
    "        n_datasets += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a29df-7717-4c8e-a356-d975a879d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.concat(urls).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7a847-1d8a-4f43-a2c8-dcd18b4c22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"ice cov\"][1]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d57b4-5410-41ac-bfd7-952101e4415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"ph\"][0]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884eaf21-56f2-4da1-8627-45b334c29369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"elevation\"][-1]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3025d87-ba7b-4aca-8828-d758da26443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"doi\"][0]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dffa8b-0075-4a63-b547-c84d51e7bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, files_with_repeat_urls[3]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28971c-98c9-4f82-b376-4614d662c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"latitude south\"][0]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da06fe0-5d27-4161-92ce-817647524b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"date\"][0]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd2af0-f9b7-46dc-80ab-fcc8ab54bf45",
   "metadata": {},
   "source": [
    "## Implement dataset cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7111b-cca5-4f29-b49b-806ca744c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXONOMY_RANKS = [\n",
    "    [\"Kingdom\", \"Regnum\"],\n",
    "    [\"Phylum\", \"Division\"],\n",
    "    [\"Ordo\", \"Order\"],\n",
    "    [\"Familia\", \"Family\"],\n",
    "    [\"Genus\"],\n",
    "    [\"Species\"],\n",
    "]\n",
    "\n",
    "\n",
    "def row2taxonomy(row):\n",
    "    parts = []\n",
    "    for rank_synonyms in TAXONOMY_RANKS:\n",
    "        for col in rank_synonyms:\n",
    "            col_ = col.lower()\n",
    "            if col in row.keys() and row[col] and row[col] != \"-\":\n",
    "                parts.append(row[col])\n",
    "                break\n",
    "            elif col_ in row.keys() and row[col_] and row[col_] != \"-\":\n",
    "                parts.append(row[col_])\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return \" > \".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1931b2-0617-421f-ae7f-ed62c2b0cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_title(title):\n",
    "    \"\"\"\n",
    "    Screen dataset title.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        The title of the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Whether the dataset title is acceptable.\n",
    "    \"\"\"\n",
    "\n",
    "    title = str(title)\n",
    "\n",
    "    if title.startswith(\"Meteorological observations\"):\n",
    "        return False\n",
    "    if title.startswith(\"Sea ice conditions\"):\n",
    "        return False\n",
    "    if \"topsoil\" in title.lower():\n",
    "        return False\n",
    "    if \"core\" in title.lower():\n",
    "        # return False\n",
    "        pass\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def add_file_extension(row):\n",
    "    \"\"\"\n",
    "    Add file extension to image filename.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : dict\n",
    "        A dict record which may have fields ``\"image\"``, ``\"File format\"``, ``\"File type\"``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fname : str\n",
    "        File name with extension included.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        \"image\" not in row.keys()\n",
    "        or not row[\"image\"]\n",
    "        or not isinstance(row[\"image\"], str)\n",
    "    ):\n",
    "        return \"\"\n",
    "\n",
    "    s = row[\"image\"]\n",
    "    ext = os.path.splitext(s)[-1]\n",
    "    if (\n",
    "        ext.lower()\n",
    "        in checker.VALID_IMG_EXTENSIONS\n",
    "        + checker.INVALID_FILE_EXTENSIONS\n",
    "        + checker.COMPRESSED_FILE_EXTENSIONS\n",
    "    ):\n",
    "        return s\n",
    "\n",
    "    for col in [\"File format\", \"File type\"]:\n",
    "        if col not in row.keys():\n",
    "            continue\n",
    "        new_ext = row[col]\n",
    "        if not new_ext or not isinstance(new_ext, str):\n",
    "            continue\n",
    "        new_ext = \".\" + new_ext.strip().lstrip(\".\")\n",
    "        if ext == new_ext:\n",
    "            break\n",
    "        s += new_ext\n",
    "        break\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def datetime2timestamp(ts):\n",
    "    \"\"\"\n",
    "    Convert a datetime string to a timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ts : str\n",
    "        Datetime string, in a format understood by ``dateutil.parser``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Timestamp; number of seconds since Unix epoch.\n",
    "    \"\"\"\n",
    "    if isinstance(ts, str):\n",
    "        return dateutil.parser.parse(ts).timestamp()\n",
    "    return ts\n",
    "\n",
    "\n",
    "def reformat_df(df, remove_duplicate_columns=True):\n",
    "    \"\"\"\n",
    "    Reformat/clean pangaea dataset.\n",
    "\n",
    "    Rename columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Original dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame or None\n",
    "        Cleaned dataset, or ``None`` if the dataset is invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    if (\n",
    "        \"dataset_title\" in df\n",
    "        and len(df) > 0\n",
    "        and df.iloc[0][\"dataset_title\"]\n",
    "        and not check_title(df.iloc[0][\"dataset_title\"])\n",
    "    ):\n",
    "        return None\n",
    "\n",
    "    # Make a copy of the dataframe so we can't overwrite the input\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove bad columns\n",
    "    df.drop(labels=[\"-\"], axis=\"columns\", inplace=True, errors=\"ignore\")\n",
    "    # Remove duplicately named columns\n",
    "    cols_to_drop = []\n",
    "    if remove_duplicate_columns:\n",
    "        for col in df.columns:\n",
    "            if len(col) < 2:\n",
    "                continue\n",
    "            if (\n",
    "                (col[-2] in \" _\")\n",
    "                and (col[-1] in \"123456789\")\n",
    "                and (col[:-2] in df.columns)\n",
    "            ):\n",
    "                cols_to_drop.append(col)\n",
    "        df.drop(labels=cols_to_drop, axis=\"columns\", inplace=True)\n",
    "\n",
    "    # Find the correct URL column, and drop other columns containing \"url\"\n",
    "    cols_to_drop = []\n",
    "    mapping = {}\n",
    "    col_url = find_url_column(df)\n",
    "    mapping[col_url] = \"url\"\n",
    "    for col in df.columns:\n",
    "        if col != col_url and \"url\" in col.lower():\n",
    "            cols_to_drop.append(col)\n",
    "\n",
    "    # Search for matches to canonical columns.\n",
    "    # Each entry in desired_columns is a key, value pair where the key\n",
    "    # is the output column name, and the value is a list of search names\n",
    "    # in order of priority. The first match will be kept and others discarded.\n",
    "    desired_columns = {\n",
    "        \"dataset\": [\"ds_id\", \"dataset\", \"Campaign\", \"campaign\"],\n",
    "        \"site\": [\"Event\", \"event\", \"Site\", \"site\", \"deployment\"],\n",
    "        \"image\": [\"image\", \"filename\"],\n",
    "        \"datetime\": [\n",
    "            \"Date/Time\",\n",
    "            \"datetime\",\n",
    "            \"timestamp\",\n",
    "            \"date/timestart\",\n",
    "            \"date/timeend\",\n",
    "            \"date\",\n",
    "        ],\n",
    "        \"latitude\": [\n",
    "            \"Latitude\",\n",
    "            \"latitude\",\n",
    "            \"lat\",\n",
    "            \"latitude+\",\n",
    "            \"latitudemed\",\n",
    "            \"latitudenorth\",\n",
    "            \"latitudesouth\",\n",
    "        ],\n",
    "        \"longitude\": [\n",
    "            \"Longitude\",\n",
    "            \"longitude\",\n",
    "            \"lon\",\n",
    "            \"long\",\n",
    "            \"longitude+\",\n",
    "            \"longitudemed\",\n",
    "            \"longitudewest\",\n",
    "            \"longitudeeast\",\n",
    "        ],\n",
    "        \"x_pos\": [],\n",
    "        \"y_pos\": [],\n",
    "        \"altitude\": [\"altitude\", \"height\"],\n",
    "        \"depth\": [\n",
    "            \"depthwater\",\n",
    "            \"bathydepth\",\n",
    "            \"bathymetry\",\n",
    "            \"bathy\",\n",
    "            \"depth\",\n",
    "            \"elevation\",\n",
    "        ],\n",
    "        \"backscatter\": [],\n",
    "        \"temperature\": [\"temperature\", \"temp\"],\n",
    "        \"salinity\": [\"salinity\", \"sal\"],\n",
    "        \"chlorophyll\": [],\n",
    "        \"acidity\": [\"pH\"],\n",
    "        \"doi\": [\"DOI\", \"doi\"],\n",
    "    }\n",
    "    # Remove non-alphanumeric padding characters, including spaces, from actual column names\n",
    "    raw_cols = list(df.columns)\n",
    "    clean_cols = [\n",
    "        col.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\").replace(\".\", \"\")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    # Map to lower case\n",
    "    lower_cols = [col.lower() for col in clean_cols]\n",
    "\n",
    "    # Search for matching column names\n",
    "    for canon, searches in desired_columns.items():\n",
    "        found = False\n",
    "\n",
    "        # Check for case-sensitive, non-alphanumeric, match\n",
    "        for search in searches:\n",
    "            if search not in raw_cols:\n",
    "                continue\n",
    "            col = search\n",
    "            if not found:\n",
    "                found = True\n",
    "                mapping[col] = canon\n",
    "                if col != canon and canon in df.columns:\n",
    "                    cols_to_drop.append(canon)\n",
    "            elif col not in mapping and col not in cols_to_drop:\n",
    "                cols_to_drop.append(col)\n",
    "\n",
    "        # Check for case-sensitive match\n",
    "        for search in searches:\n",
    "            if search not in clean_cols:\n",
    "                continue\n",
    "            col = df.columns[clean_cols.index(search)]\n",
    "            if not found:\n",
    "                found = True\n",
    "                mapping[col] = canon\n",
    "                if col != canon and canon in df.columns:\n",
    "                    cols_to_drop.append(canon)\n",
    "            elif col not in mapping and col not in cols_to_drop:\n",
    "                cols_to_drop.append(col)\n",
    "\n",
    "        # Check for case-insensitive match\n",
    "        for search in searches:\n",
    "            if search.lower() not in lower_cols:\n",
    "                continue\n",
    "            col = df.columns[lower_cols.index(search.lower())]\n",
    "            if not found:\n",
    "                found = True\n",
    "                mapping[col] = canon\n",
    "                if col != canon and canon in df.columns:\n",
    "                    cols_to_drop.append(canon)\n",
    "            elif col not in mapping and col not in cols_to_drop:\n",
    "                cols_to_drop.append(col)\n",
    "\n",
    "    # Remove superfluous columns\n",
    "    df.drop(labels=cols_to_drop, axis=\"columns\", inplace=True)\n",
    "    # Rename columns to canonical names\n",
    "    df.rename(columns=mapping, inplace=True, errors=\"raise\")\n",
    "\n",
    "    # Add file extension to image\n",
    "    df[\"image\"] = df.apply(add_file_extension, axis=1)\n",
    "    # if \"timestamp\" not in df.columns and \"datetime\" in df.columns:\n",
    "    #     df[\"timestamp\"] = df[\"datetime\"].apply(datetime2timestamp)\n",
    "\n",
    "    if any([c in clean_cols for c in [\"Kingdom\", \"Phylum\", \"Genus\"]]):\n",
    "        df[\"taxonomy\"] = df.apply(row2taxonomy, axis=1)\n",
    "        df.drop(\n",
    "            labels=[x for syn in TAXONOMY_RANKS for x in syn],\n",
    "            axis=\"columns\",\n",
    "            inplace=True,\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "\n",
    "    cols_to_drop = [\n",
    "        \"File format\",\n",
    "        \"File type\",\n",
    "        \"File size\",\n",
    "        \"Date/Time\",\n",
    "        \"Date/time end\",\n",
    "    ]\n",
    "    df.drop(labels=cols_to_drop, axis=\"columns\", inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffdb5b-725a-4679-9031-68669e298d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_title(\"Sea ice conditions at location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21314a94-a03c-4317-87fb-0578e3f04381",
   "metadata": {},
   "source": [
    "## Load data with dataset cleaning functions applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed7c35-614c-40b8-a192-e3ef24b984cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_count = defaultdict(lambda: 0)\n",
    "column_examples = defaultdict(lambda: [])\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "dfs = []\n",
    "dfs_fnames = []\n",
    "\n",
    "for fname in tqdm(os.listdir(dirname)):\n",
    "    ds_id = os.path.splitext(fname)[0]\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    n_total += 1\n",
    "    if not checker.has_url_col(df):\n",
    "        continue\n",
    "\n",
    "    url_col = find_url_column(df)\n",
    "    if not url_col:\n",
    "        files_without_url.append(fname)\n",
    "        continue\n",
    "\n",
    "    df[\"ds_id\"] = f\"pangaea-{ds_id}\"\n",
    "    df = reformat_df(df)\n",
    "    if df is None:\n",
    "        continue\n",
    "    url_col = \"url\"\n",
    "    df = df[df[url_col] != \"\"]\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "\n",
    "    n_valid += 1\n",
    "\n",
    "    for col in df.columns:\n",
    "        column_count[col] += 1\n",
    "        column_examples[col].append(fname)\n",
    "\n",
    "    if len(df) != len(df.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)\n",
    "\n",
    "    dfs.append(df)\n",
    "    dfs_fnames.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e5487-685f-49ec-aac3-2af68b942a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) valid datasets\")\n",
    "print(f\"Of which {len(files_with_repeat_urls)} have repeated URLs\")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315634da-ccc1-45f5-9eff-c298c518e765",
   "metadata": {},
   "source": [
    "### Merge datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666b6ba-7513-41fc-b504-76b40d6e9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cols = {\n",
    "    \"dataset\",\n",
    "    \"site\",\n",
    "    \"url\",\n",
    "    \"image\",\n",
    "    \"datetime\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"x_pos\",\n",
    "    \"y_pos\",\n",
    "    \"altitude\",\n",
    "    \"depth\",\n",
    "    \"backscatter\",\n",
    "    \"temperature\",\n",
    "    \"salinity\",\n",
    "    \"chlorophyll\",\n",
    "    \"acidity\",\n",
    "}\n",
    "\n",
    "df_all = pd.concat([df[df.columns.intersection(select_cols)] for df in dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa83a9-05fd-4161-be11-3f6369f8a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eddfd9-5541-4681-8da5-8ec4b8950eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all records\n",
    "df_all.to_csv(\n",
    "    f\"../pangaea_{datetime.datetime.today().strftime('%Y-%m-%d')}.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082806f5-8aeb-4b97-8e39-c5749b3b7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only valid URLs\n",
    "df_all = df_all[df_all[\"url\"].apply(checker.is_url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4a895-2086-49c4-8c67-1b1930b76c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ffef8-ff55-48f6-b588-ffbdf036145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:4])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f7f88-d2a5-43b5-9190-5af4a6c058e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8dd81e-1126-4bb8-a2bd-2c66c6785da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = unique_url_bases[0]\n",
    "df_all[df_all[\"url\"].str.startswith(url_base)].iloc[[0, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d919c8d-136e-4d1f-8275-caf41d17b8b5",
   "metadata": {},
   "source": [
    "### Find bad subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8106df-d060-43ec-b91e-3a23fa7ee1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c7b28-64fb-4950-8082-94f884f329ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows have lat & lon\n",
    "sum(~df_all[\"latitude\"].isna() & ~df_all[\"longitude\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a511931-45ab-46a6-a37c-337ee9324fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837620f-29ba-421b-93ae-1ff1dda78b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate URLs\n",
    "df_all = df_all.drop_duplicates(subset=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98d486-cf02-4797-98e4-dafaed33a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37dcd1b-345c-4a9a-8c0a-916913f296c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows have lat & lon\n",
    "sum(~df_all[\"latitude\"].isna() & ~df_all[\"longitude\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456fb65-4762-4d27-9585-abf6d23cead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693c9c0-96d4-4006-b3cc-41bfb959a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_image = df_all[\"url\"].apply(\n",
    "    lambda x: checker.has_img_extension(x.rstrip(\"/\"))\n",
    ") | df_all[\"image\"].apply(lambda x: checker.has_img_extension(x.rstrip(\"/\")))\n",
    "df_all = df_all[is_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb88409-50aa-463c-824f-c74d76180271",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d8862-607e-48f7-b8ff-dfa4fa54be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(~df_all[\"latitude\"].isna() & ~df_all[\"longitude\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807fed2-52b0-4f7c-97c9-9833679fd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f48d22-7fdd-4d45-80e0-7413d2568df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:4])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46de5eb-5a71-4ce2-9f12-139a482c8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a0418-aa84-41a9-94b9-2780d7eba123",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i + 1, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5b1f9-9148-4858-a1ed-d5a5d225f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subdomain(url):\n",
    "    blacklist = [\n",
    "        \"https://doi.org/10.1594/PANGAEA\",\n",
    "        \"http://epic.awi.de/\",\n",
    "        \"https://epic.awi.de/\",\n",
    "        \"http://hdl.handle.net/10013/\",\n",
    "        \"http://library.ucsd.edu/dc/object/\",\n",
    "        \"https://hs.pangaea.de/Maps/\",\n",
    "        \"https://hs.pangaea.de/Movies/\",\n",
    "        \"https://hs.pangaea.de/Projects/\",\n",
    "        \"https://hs.pangaea.de/bathy/\",\n",
    "        \"https://hs.pangaea.de/fishsounder/\",\n",
    "        \"https://hs.pangaea.de/mag/\",\n",
    "        \"https://hs.pangaea.de/model/\",\n",
    "        \"https://hs.pangaea.de/nav/\",\n",
    "        \"https://hs.pangaea.de/palaoa/\",\n",
    "        \"https://hs.pangaea.de/para/\",\n",
    "        \"https://hs.pangaea.de/reflec/\",\n",
    "        \"https://hs.pangaea.de/sat/\",\n",
    "        \"https://prr.osu.edu/collection/object/\",\n",
    "        \"https://store.pangaea.de/Projects/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/Publications/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/software/\",\n",
    "        \"https://www.ngdc.noaa.gov/geosamples/\",\n",
    "    ]\n",
    "    for entry in blacklist:\n",
    "        if url.startswith(entry):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d54c9-6f69-4229-b5ec-ad62d4876dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad URLs based on their subdomain\n",
    "df_all = df_all[df_all[\"url\"].apply(check_subdomain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd747c-1bdb-43eb-a5bd-af83bfb7494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797d5fb-929a-48f1-a55c-985bfdcbc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:5])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15212b59-e4c5-4d23-ba2c-7b9cc597400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6b436-d7ee-4748-b609-869bff5bd7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i + 1, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce63a9-4e1b-44a5-b95f-5d88298ce878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d14f94-f85e-4a74-b589-1e99ed53facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.search(\n",
    "    \"(?<![A-Za-z])map(?![A-Za-z])\",\n",
    "    \"https://hs.pangaea.de/Images/ROV/M/M114/GeoB19346-1/data_publish/config/divemap/Dive360_map.jpg\",\n",
    "):\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b03137-b9f3-4e4c-b46d-1a2c62430112",
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.search(\n",
    "    \"(?<![A-Za-z])map(?![A-Za-z])\",\n",
    "    \"https://hs.pangaea.de/Images/ROV/M/M114/GeoB19346-1/data_publish/config/divemap/Dive360mapy.jpg\",\n",
    "):\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd73bc0-7f1f-4aec-bca0-b4ee872a9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subdomain(url):\n",
    "    blacklist = [\n",
    "        \"https://doi.org/10.1594/PANGAEA\",\n",
    "        \"http://epic.awi.de/\",\n",
    "        \"https://epic.awi.de/\",\n",
    "        \"http://hdl.handle.net/10013/\",\n",
    "        \"http://library.ucsd.edu/dc/object/\",\n",
    "        \"https://app.geosamples.org/uploads/UHM\",\n",
    "        \"https://hs.pangaea.de/Images/Linescan\",\n",
    "        \"https://hs.pangaea.de/Maps/\",\n",
    "        \"https://hs.pangaea.de//Maps\",\n",
    "        \"https://hs.pangaea.de/Movies/\",\n",
    "        \"https://hs.pangaea.de/Projects/\",\n",
    "        \"https://hs.pangaea.de/bathy/\",\n",
    "        \"https://hs.pangaea.de/fishsounder/\",\n",
    "        \"https://hs.pangaea.de/mag/\",\n",
    "        \"https://hs.pangaea.de/model/\",\n",
    "        \"https://hs.pangaea.de/nav/\",\n",
    "        \"https://hs.pangaea.de/palaoa/\",\n",
    "        \"https://hs.pangaea.de/pasata/\",\n",
    "        \"https://hs.pangaea.de/para/\",\n",
    "        \"https://hs.pangaea.de/polar\",\n",
    "        \"https://hs.pangaea.de/reflec/\",\n",
    "        \"https://hs.pangaea.de/sat/\",\n",
    "        \"https://prr.osu.edu/collection/object/\",\n",
    "        \"https://store.pangaea.de/Projects/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/Publications/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/software/\",\n",
    "        \"https://www.ngdc.noaa.gov/geosamples/\",\n",
    "        \"https://hs.pangaea.de/Images/Airphoto\",\n",
    "        # \"https://hs.pangaea.de/Images/Cores\",  # Some of these are okay\n",
    "        \"https://hs.pangaea.de/Images/Documentation\",\n",
    "        \"https://hs.pangaea.de/Images/Maps\",\n",
    "        \"https://hs.pangaea.de/Images/MMT/\",\n",
    "        \"https://hs.pangaea.de/Images/Plankton\",\n",
    "        # The GeoB19346-1 dataset contains .bmp images of the ROV's sonar\n",
    "        \"https://hs.pangaea.de/Images/ROV/M/M114/GeoB19346-1/data_publish/data/sonar/\",\n",
    "        \"https://hs.pangaea.de/Images/Satellite\",\n",
    "        \"https://hs.pangaea.de/Images/SeaIce\",\n",
    "        \"https://hs.pangaea.de/Images/Water\",\n",
    "        \"https://store.pangaea.de/Images/Airphoto\",\n",
    "        \"https://store.pangaea.de/Images/Documentation\",\n",
    "    ]\n",
    "    banned_words = [\"map\", \"divemap\", \"dredge_photos\", \"dredgephotograph\"]\n",
    "    for entry in blacklist:\n",
    "        if url.startswith(entry):\n",
    "            return False\n",
    "    for word in banned_words:\n",
    "        if re.search(\"(?<![A-Za-z])\" + word + \"(?![A-Za-z])\", url.lower()):\n",
    "            return False\n",
    "    if re.search(\"(?<![a-z])core(?![a-rty])\", url.lower()) and \"SUR\" not in url:\n",
    "        # Images of cores must contain \"SURFACE\", or the shorthand \"SUR\"\n",
    "        # We only keep the ones with surface in uppercase, because those\n",
    "        # experiments are in-situ surface photos, whereas lower case are not.\n",
    "        return False\n",
    "    if \"not_available\" in url:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7fb7d-f80a-4788-bb22-102419e32be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad URLs based on their subdomain (again), and remove maps\n",
    "df_all = df_all[df_all[\"url\"].apply(check_subdomain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f68e32-c94a-46a5-b1ec-330b2fe182a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39795a9-49a1-4d54-b3fa-0532c403b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:5])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db689e-1288-405e-bfb6-89e58a91ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f6870-57fd-4884-84c9-34ef5df34e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i + 1, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[499])\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[4999])\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7a1c8-920f-4d26-823a-da683f8e9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all[\"url\"].apply(lambda x: \"mosaic\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037953bb-6831-4f53-806d-254b313604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop mosaic images\n",
    "df_all = df_all[~df_all[\"url\"].apply(lambda x: \"mosaic\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da8200-c0f8-4aa8-baf2-8d95d0a52077",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_extensions = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: os.path.splitext(x)[1]).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9df6bc-5a67-4ffa-bc55-de1d855ff971",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a71797e-8da2-497b-ba20-dc4101d18c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ext in enumerate(unique_extensions):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.endswith(ext)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), extension {}\".format(\n",
    "            i + 1, len(unique_extensions), len(sdf), ext\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[499])\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[4999])\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08727d-7895-41a2-8cfc-c1eecd4641e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954df08d-4934-4a40-9268-c26ac162f19c",
   "metadata": {},
   "source": [
    "### Save unlabelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9bde3-cbda-402d-ba37-122b0507b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\n",
    "    f\"../pangaea_{datetime.datetime.today().strftime('%Y-%m-%d')}_filtered.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd024ef3-6472-4d12-9a87-a75f03a7e54e",
   "metadata": {},
   "source": [
    "### Check images for each title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb11ae-e912-4f74-a7db-d29188c5db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "_dfs = []\n",
    "_dfs_fnames = []\n",
    "\n",
    "for df, df_fname in tqdm(zip(dfs, dfs_fnames), total=len(dfs)):\n",
    "    url_column = \"url\"\n",
    "\n",
    "    # Filter down to only valid URLs\n",
    "    df = df[df[url_column].apply(checker.is_url)]\n",
    "    if df is None or len(df) == 0:\n",
    "        continue\n",
    "\n",
    "    # Remove bad subdomains\n",
    "    df = df[df[url_column].apply(check_subdomain)]\n",
    "    if df is None or len(df) == 0:\n",
    "        continue\n",
    "\n",
    "    # Filter down to only rows which have image extension\n",
    "    is_image = df[url_column].apply(lambda x: checker.has_img_extension(x.rstrip(\"/\")))\n",
    "    if \"image\" in df.columns:\n",
    "        is_image |= df[\"image\"].apply(\n",
    "            lambda x: checker.has_img_extension(x.rstrip(\"/\"))\n",
    "        )\n",
    "    df = df[is_image]\n",
    "    if df is None or len(df) == 0:\n",
    "        continue\n",
    "\n",
    "    _dfs.append(df)\n",
    "    _dfs_fnames.append(df_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3f4ef-e765-4939-8a21-6e6a913fd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Keeping {len(_dfs)} of {len(dfs)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f782622-6cfd-4744-b19d-5805282fb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = _dfs\n",
    "dfs_fnames = _dfs_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2da16f-0ee4-476d-9de2-a626163d6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_length = 40\n",
    "title_starters_to_row = {}\n",
    "\n",
    "for df, fname in zip(dfs, dfs_fnames):\n",
    "    if \"dataset_title\" not in df.columns:\n",
    "        continue\n",
    "    for title in df[\"dataset_title\"].unique():\n",
    "        if pd.isna(title):\n",
    "            continue\n",
    "        if title[:starter_length] in title_starters_to_row:\n",
    "            continue\n",
    "        subdf = df[df[\"dataset_title\"] == title]\n",
    "        title_starters_to_row[title[:starter_length]] = subdf.iloc[len(subdf) // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4071a-22f2-4c2e-9b4b-dc627b56fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(title_starters_to_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd73eeb-384f-4d0b-8361-124d7b3d5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(title_starters_to_row.keys()):\n",
    "    print(f\"{k:40} ... {title_starters_to_row[k]['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67f17b-d257-4687-b2dc-8237a514cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(title_starters_to_row.keys()):\n",
    "    print(title_starters_to_row[k][\"dataset_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b297235-d351-456a-8055-735e07e4263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:6])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd623d-51c5-4370-afc1-c4ffd1c49c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfea66-dc66-4a4d-bd32-a4e5026ed734",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = [\n",
    "    x\n",
    "    for x in unique_url_bases\n",
    "    if x.startswith(\"https://hs.pangaea.de/bathy\")\n",
    "    or x.startswith(\"https://hs.pangaea.de/Images\")\n",
    "    or x.startswith(\"https://store.pangaea.de/Images\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a071c-6b4d-4b60-a38e-acabc3c88c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edde3bf-6fbe-4b3e-8185-1d7c37d99713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_url_bases\n",
    "\n",
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i + 1, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[499])\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[4999])\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffaacf-79ed-451b-b567-9c17f72c8c33",
   "metadata": {},
   "source": [
    "#### Refine title and subdomain screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe062b59-e6c0-418c-9a9e-353c2d32aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_title(title):\n",
    "    \"\"\"\n",
    "    Screen dataset title.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        The title of the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Whether the dataset title is acceptable.\n",
    "    \"\"\"\n",
    "\n",
    "    title = str(title)\n",
    "\n",
    "    if \"do not use\" in title.lower():\n",
    "        return False\n",
    "    if title.startswith(\"Meteorological observations\"):\n",
    "        return False\n",
    "    if title.startswith(\"Sea ice conditions\"):\n",
    "        return False\n",
    "    if \"topsoil\" in title.lower():\n",
    "        return False\n",
    "    if \"core\" in title.lower():\n",
    "        # return False\n",
    "        pass\n",
    "    if \"aquarium\" in title.lower():\n",
    "        return False\n",
    "    if \" of the early life history \" in title.lower():\n",
    "        return False\n",
    "    if \"grab sample\" in title.lower():\n",
    "        return False\n",
    "    if title.startswith(\"Calyx growth\"):\n",
    "        return False\n",
    "    if \"dried glass sponges\" in title.lower():\n",
    "        return False\n",
    "    if \"fresh glass sponges\" in title.lower():\n",
    "        return False\n",
    "    if \"spicule preparations\" in title.lower():\n",
    "        return False\n",
    "    if title.startswith(\"Shell growth increments\"):\n",
    "        return False\n",
    "    if title.startswith(\"Images of shell cross sections\"):\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736da96c-e304-4a81-b7f0-34568dca9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_title(\"Grab samples from location\")  # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40c6f49-f76a-4f54-b954-87951802c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subdomain(url):\n",
    "    blacklist = [\n",
    "        \"https://doi.org/10.1594/PANGAEA\",\n",
    "        \"http://epic.awi.de/\",\n",
    "        \"https://epic.awi.de/\",\n",
    "        \"http://hdl.handle.net/10013/\",\n",
    "        \"http://library.ucsd.edu/dc/object/\",\n",
    "        \"https://app.geosamples.org/uploads/UHM/\",\n",
    "        \"https://hs.pangaea.de/Images/Linescan/\",\n",
    "        \"https://hs.pangaea.de/Maps/\",\n",
    "        \"https://hs.pangaea.de//Maps\",\n",
    "        \"https://hs.pangaea.de/Movies/\",\n",
    "        \"https://hs.pangaea.de/Projects/\",\n",
    "        \"https://hs.pangaea.de/bathy/\",\n",
    "        \"https://hs.pangaea.de/fishsounder/\",\n",
    "        \"https://hs.pangaea.de/mag/\",\n",
    "        \"https://hs.pangaea.de/model/\",\n",
    "        \"https://hs.pangaea.de/nav/\",\n",
    "        \"https://hs.pangaea.de/palaoa/\",\n",
    "        \"https://hs.pangaea.de/pasata/\",\n",
    "        \"https://hs.pangaea.de/para/\",\n",
    "        \"https://hs.pangaea.de/polar\",\n",
    "        \"https://hs.pangaea.de/reflec/\",\n",
    "        \"https://hs.pangaea.de/sat/\",\n",
    "        \"https://prr.osu.edu/collection/object/\",\n",
    "        \"https://store.pangaea.de/Projects/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/Publications/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/software/\",\n",
    "        \"https://www.ngdc.noaa.gov/geosamples/\",\n",
    "        \"https://hs.pangaea.de/Images/Airphoto/\",\n",
    "        # \"https://hs.pangaea.de/Images/Cores/\",  # Some of these are okay\n",
    "        \"https://hs.pangaea.de/Images/Documentation/\",\n",
    "        \"https://hs.pangaea.de/Images/Maps/\",\n",
    "        \"https://hs.pangaea.de/Images/MMT/\",\n",
    "        \"https://hs.pangaea.de/Images/Plankton/\",\n",
    "        # The GeoB19346-1 dataset contains .bmp images of the ROV's sonar\n",
    "        \"https://hs.pangaea.de/Images/ROV/M/M114/GeoB19346-1/data_publish/data/sonar/\",\n",
    "        \"https://hs.pangaea.de/Images/Satellite/\",\n",
    "        \"https://hs.pangaea.de/Images/SeaIce/\",\n",
    "        \"https://hs.pangaea.de/Images/Water/\",\n",
    "        \"https://store.pangaea.de/Images/Airphoto/\",\n",
    "        \"https://store.pangaea.de/Images/Documentation/\",\n",
    "        \"https://hs.pangaea.de/Images/Benthos/AWI_experimental_aquarium_system/\",\n",
    "        # \"https://hs.pangaea.de/Images/Benthos/AntGlassSponges\",  # Only okay if it contains \"AHEAD\"\n",
    "        \"https://hs.pangaea.de/Images/Benthos/Kongsfjorden/MHerrmann/\",  # Microscope images\n",
    "        \"https://hs.pangaea.de/Images/Benthos/Kongsfjorden/Brandal/\",  # Cross-sections\n",
    "    ]\n",
    "    banned_words = [\n",
    "        \"aquarium\",\n",
    "        \"map\",\n",
    "        \"divemap\",\n",
    "        \"dredgephotos\",\n",
    "        \"dredge_photos\",\n",
    "        \"dredgephotograph\",\n",
    "        \"grabsample\",\n",
    "        \"grab_sample\",\n",
    "    ]\n",
    "    for entry in blacklist:\n",
    "        if url.startswith(entry):\n",
    "            return False\n",
    "    for word in banned_words:\n",
    "        if re.search(\"(?<![A-Za-z])\" + word + \"(?![A-Za-z])\", url.lower()):\n",
    "            return False\n",
    "    if re.search(\"(?<![a-z])core(?![a-rty])\", url.lower()) and \"SUR\" not in url:\n",
    "        # Images of cores must contain \"SURFACE\", or the shorthand \"SUR\"\n",
    "        # We only keep the ones with surface in uppercase, because those\n",
    "        # experiments are in-situ surface photos, whereas lower case are not.\n",
    "        return False\n",
    "    if (\n",
    "        url.startswith(\"https://hs.pangaea.de/Images/Benthos/AntGlassSponges/\")\n",
    "        and \"AHEAD\" not in url\n",
    "    ):\n",
    "        # Images of AntGlassSponges must contain \"AHEAD\" to be kept\n",
    "        # otherwise, they are of sponges after removal\n",
    "        return False\n",
    "    if \"not_available\" in url:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee172d3-7d82-4e12-ab99-5de0f376face",
   "metadata": {},
   "source": [
    "## Load data with URL filtering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c51ae6-06e3-4807-84dd-57197308e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_urls(df, url_column=\"url\", inplace=True):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "\n",
    "    # Filter down to only valid URLs\n",
    "    df = df[df[url_column].apply(checker.is_url)]\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "\n",
    "    # Remove bad subdomains\n",
    "    df = df[df[url_column].apply(check_subdomain)]\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "\n",
    "    # Filter down to only rows which have image extension\n",
    "    is_image = df[url_column].apply(lambda x: checker.has_img_extension(x.rstrip(\"/\")))\n",
    "    if \"image\" in df.columns:\n",
    "        is_image |= df[\"image\"].apply(\n",
    "            lambda x: checker.has_img_extension(x.rstrip(\"/\"))\n",
    "        )\n",
    "    df = df[is_image]\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "\n",
    "    # Drop mosaic images\n",
    "    df = df[~df[url_column].apply(lambda x: \"mosaic\" in x.lower())]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4838b1-ddc5-4979-9577-b78b9c72c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixup_url_with_image(row):\n",
    "    url_old = row[\"url\"]\n",
    "    if \"image\" not in row or not row[\"image\"]:\n",
    "        return url_old\n",
    "    url = url_old.rstrip(\"/ \")\n",
    "    url_parts = url.split(\"/\")\n",
    "    ext = os.path.splitext(url_parts[-1])[-1]\n",
    "    url_new = \"/\".join(url_parts[:-1])\n",
    "    if row[\"image\"] + ext == url_parts[-1]:\n",
    "        return url\n",
    "    url_new += \"/\" + row[\"image\"]\n",
    "    return url_new\n",
    "\n",
    "\n",
    "def insert_rows(df, rows, indices):\n",
    "    parts = [df[: indices[0]], pd.DataFrame([rows[0]])]\n",
    "    for j in range(len(indices) - 1):\n",
    "        parts.append(df[indices[j - 1] : indices[j]])\n",
    "        parts.append(pd.DataFrame([rows[j]]))\n",
    "    parts.append(df[indices[-1] :])\n",
    "    return pd.concat(parts)\n",
    "\n",
    "\n",
    "def fixup_repeated_urls(\n",
    "    df, url_column=\"url\", inplace=True, force_keep_original=True, verbose=1\n",
    "):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    dup_urls = df[df.duplicated(subset=url_column)][url_column].unique()\n",
    "    if \"image\" not in df.columns:\n",
    "        if len(dup_urls) and verbose >= 1:\n",
    "            print(\n",
    "                f\"Can't clean {len(dup_urls)} repeated URLs in {df.loc[0, 'ds_id']} without 'image' column\"\n",
    "            )\n",
    "        return df\n",
    "    if len(dup_urls) == 0:\n",
    "        return df\n",
    "    if verbose >= 1 and \"dataset\" in df.columns:\n",
    "        print(f\"{df.iloc[0]['dataset']} has {len(dup_urls)} duplicated URLs\")\n",
    "    rows_to_insert = []\n",
    "    indices_to_insert_at = []\n",
    "    for dup_url in dup_urls:\n",
    "        if pd.isna(dup_url):\n",
    "            continue\n",
    "        is_bad_url = df[url_column] == dup_url\n",
    "        if verbose >= 2:\n",
    "            n_to_change = sum(is_bad_url)\n",
    "            print(f\"Fixing up {n_to_change} repetitions of the URL {dup_url}\")\n",
    "        first_row_idx = np.nonzero(is_bad_url.values)[0][0]\n",
    "        if force_keep_original:\n",
    "            # first_row = df[is_bad_url].iloc[0].copy()\n",
    "            first_row = df.iloc[first_row_idx].copy()\n",
    "        df.loc[is_bad_url, url_column] = df[is_bad_url].apply(\n",
    "            fixup_url_with_image, axis=1\n",
    "        )\n",
    "        n_remain = sum(df.loc[is_bad_url, url_column] == dup_url)\n",
    "        if verbose >= 2:\n",
    "            if n_remain == n_to_change:\n",
    "                print(f\"  All {n_to_change} URLs left unchanged\")\n",
    "            else:\n",
    "                print(f\"  {n_remain} / {n_to_change} URLs remain unchanged\")\n",
    "                if verbose >= 3:\n",
    "                    print(\"  After:\")\n",
    "                    for x in df.loc[is_bad_url, url_column][:5]:\n",
    "                        print(f\"    {x}\")\n",
    "                    if n_to_change > 5:\n",
    "                        print(\"    ...\")\n",
    "                        print(\"    \" + df.loc[is_bad_url, url_column].values[-1])\n",
    "\n",
    "        if force_keep_original and n_remain == 0:\n",
    "            if verbose >= 1:\n",
    "                print(f\"  Duplicating a row since the URL {dup_url} no longer appears\")\n",
    "            first_row[url_column] = dup_url\n",
    "            rows_to_insert.append(first_row)\n",
    "            indices_to_insert_at.append(first_row_idx)\n",
    "    if len(rows_to_insert) > 0:\n",
    "        if verbose >= 1:\n",
    "            print(f\"  Inserting {len(rows_to_insert)} duplicated rows\")\n",
    "        df = insert_rows(df, rows_to_insert, indices_to_insert_at)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95630a1b-e47c-40ab-a497-2491fc65e373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_count = defaultdict(lambda: 0)\n",
    "column_examples = defaultdict(lambda: [])\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "files_with_repeat_urls2 = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "dfs = []\n",
    "dfs_fnames = []\n",
    "\n",
    "for fname in tqdm(sorted(sorted(os.listdir(dirname)), key=len)):\n",
    "    # for fname in tqdm(os.listdir(dirname)):\n",
    "    ds_id = os.path.splitext(fname)[0]\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    n_total += 1\n",
    "    if not checker.has_url_col(df):\n",
    "        continue\n",
    "\n",
    "    url_col = find_url_column(df)\n",
    "    if not url_col:\n",
    "        files_without_url.append(fname)\n",
    "        continue\n",
    "\n",
    "    df[\"ds_id\"] = f\"pangaea-{ds_id}\"\n",
    "    df = reformat_df(df)\n",
    "    if df is None:\n",
    "        continue\n",
    "\n",
    "    url_col = \"url\"\n",
    "    df = df[df[url_col] != \"\"]\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "\n",
    "    df = filter_urls(df, url_column=url_col)\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "\n",
    "    n_valid += 1\n",
    "\n",
    "    for col in df.columns:\n",
    "        column_count[col] += 1\n",
    "        column_examples[col].append(fname)\n",
    "\n",
    "    # Drop rows that are complete duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    if len(df) != len(df.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)\n",
    "\n",
    "    # Try to fix repeated URLs that are accidental dups but should differ\n",
    "    df = fixup_repeated_urls(df, url_column=url_col, verbose=1)\n",
    "\n",
    "    if len(df) != len(df.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls2.append(fname)\n",
    "\n",
    "    # Check for any rows that are all NaNs\n",
    "    if sum(df.isna().all(\"columns\")) > 0:\n",
    "        print(f\"{ds_id} has a row which is all NaNs\")\n",
    "\n",
    "    dfs.append(df)\n",
    "    dfs_fnames.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42118a62-9b2a-4eda-bb37-ee0e805d6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) valid datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (before replacing dups with image)\"\n",
    ")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls2)} have repeated URLs (after replacing dups with image)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dda41e-720e-43c0-9228-485757b38422",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cols = {\n",
    "    \"dataset\",\n",
    "    \"site\",\n",
    "    \"url\",\n",
    "    \"image\",\n",
    "    \"datetime\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"altitude\",\n",
    "    \"depth\",\n",
    "    \"backscatter\",\n",
    "    \"temperature\",\n",
    "    \"salinity\",\n",
    "    \"chlorophyll\",\n",
    "    \"acidity\",\n",
    "}\n",
    "\n",
    "df_all = pd.concat(\n",
    "    [df[df.columns.intersection(select_cols)] for df in dfs if len(df) > 0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53698b76-ae55-4b69-a267-2d3363980f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73081233-3c60-44ef-8c7c-a43fb45d2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate URLs\n",
    "df_all.drop_duplicates(subset=\"url\", inplace=True, keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a95594-125b-44ba-9ef9-e1af7d382f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386bb255-8bd9-48b4-a736-af6d0f67a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[~df_all[\"url\"].apply(checker.is_url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9339c3c-cb9a-41d4-9615-e87b9418a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all.isna().all(\"columns\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3437d-9a5a-4268-b225-40105f1edac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_extensions = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: os.path.splitext(x)[1]).unique()\n",
    ")\n",
    "unique_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee5399-a89f-41d8-bf6d-906ccce041ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\n",
    "    df_all[\"url\"]\n",
    "    == \"https://hs.pangaea.de/Images/Benthos/Great_Barrier_Reef/CCMR/Opal_Reef_2017-01/20170126_Opal_S_C14_289.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70dd2a-cfdb-4e50-b56d-41b8ef1ce5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\n",
    "    df_all[\"url\"]\n",
    "    == \"https://hs.pangaea.de/Images/Benthos/Great_Barrier_Reef/CCMR/Opal_Reef_2017-01/20170126_Opal_DC2-002.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d826e-b94b-4832-9c20-fa3aa0e37f77",
   "metadata": {},
   "source": [
    "### Fix repeated output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be207e2d-6842-4bc9-bb71-7955b1661ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row2basename(row):\n",
    "    basename = row[\"image\"]\n",
    "    if pd.isna(basename) or not basename:\n",
    "        if pd.isna(row[\"url\"]):\n",
    "            return \"\"\n",
    "        basename = row[\"url\"].rstrip(\"/\").split(\"/\")[-1]\n",
    "    basename = sanitize_filename(basename)\n",
    "    ext = os.path.splitext(basename)[1]\n",
    "    expected_ext = os.path.splitext(row[\"url\"].rstrip(\"/\"))[1]\n",
    "    if expected_ext and ext.lower() != expected_ext.lower():\n",
    "        if ext.lower() in {\".jpg\", \".jpeg\"}:\n",
    "            basename = os.path.splitext(basename)[0] + expected_ext\n",
    "        else:\n",
    "            basename = basename + expected_ext\n",
    "    return basename\n",
    "\n",
    "\n",
    "def determine_outpath(df):\n",
    "    return (\n",
    "        sanitize_filename_series(df[\"dataset\"])\n",
    "        + \"/\"\n",
    "        + sanitize_filename_series(df[\"site\"])\n",
    "        + \"/\"\n",
    "        + df.apply(row2basename, axis=1)\n",
    "    )\n",
    "\n",
    "\n",
    "def fixup_repeated_output_paths(df, inplace=True, verbose=1):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    df[\"_outpath\"] = determine_outpath(df)\n",
    "    dup_outpaths = df[df.duplicated(subset=\"_outpath\")][\"_outpath\"].unique()\n",
    "\n",
    "    if len(dup_outpaths) == 0:\n",
    "        return df\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print(\n",
    "            f\"There are {len(dup_outpaths)} duplicated output paths in this dataframe\"\n",
    "        )\n",
    "\n",
    "    for dup_outpath in dup_outpaths:\n",
    "        is_bad = df[\"_outpath\"] == dup_outpath\n",
    "        n_bad = sum(is_bad)\n",
    "\n",
    "        if verbose >= 2:\n",
    "            print(f\"Trying to fix up {n_bad} repetitions of the path {dup_outpath}\")\n",
    "\n",
    "        # 1. Try taking the basename from the URL instead\n",
    "        # We will try taking just the last bit (photoname.jpg), then including preceding bits\n",
    "        # of the URL (subsite/photoname.jpg, site/subsite/photoname.jpg)\n",
    "        subdf = df[is_bad]\n",
    "        resolved = False\n",
    "        for k in range(1, 5):\n",
    "            new_basenames = subdf[\"url\"].apply(\n",
    "                lambda x: sanitize_filename(\"-\".join(x.rstrip(\"/\").split(\"/\")[-k:]))\n",
    "            )\n",
    "            # All URL basenames are unique, so we are done\n",
    "            if len(new_basenames.unique()) != len(new_basenames):\n",
    "                continue\n",
    "            df.loc[is_bad, \"image\"] = new_basenames\n",
    "            if verbose >= 2:\n",
    "                print(f\"  Using last {k} part(s) of the URL as the basename\")\n",
    "            resolved = True\n",
    "            break\n",
    "        if resolved:\n",
    "            continue\n",
    "\n",
    "        # 2. If that didn't work, just append _0, _1, ... _N to the image names\n",
    "        if verbose >= 2:\n",
    "            print(\"  Appending a suffix to the basename to prevent collisions\")\n",
    "        new_basenames = subdf.apply(row2basename, axis=1)\n",
    "        new_basenames = (\n",
    "            new_basenames.apply(lambda x: os.path.splitext(x)[0])\n",
    "            + \"_\"\n",
    "            + pd.Series([str(x) for x in range(n_bad)], index=new_basenames.index)\n",
    "            + new_basenames.apply(lambda x: os.path.splitext(x)[1])\n",
    "        )\n",
    "        df.loc[is_bad, \"image\"] = new_basenames\n",
    "\n",
    "    df.drop(columns=\"_outpath\", inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd23ef-9d41-45d2-ba53-1c3899798d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = fixup_repeated_output_paths(df_all, inplace=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c167fc-4d45-4323-9ed4-62015ee9c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ab29ba-c1b1-4997-b0f9-093553530ab5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save unlabelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cba273-ef16-4bec-bcb0-a83d1882823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\n",
    "    f\"../pangaea_{datetime.datetime.today().strftime('%Y-%m-%d')}_filtered_no-repeats_sorted-first.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9f49f-fa72-4175-bd09-231db4f45cf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Labelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a26fca-abd4-4cad-8575-dc66b1842ae9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Percent Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3e1c3-7065-428a-b748-68f1ea8e1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_dfs = [dfs[dfs_fnames.index(fname)] for fname in coverage_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca4fa07-cc87-4480-8e0f-2060168447ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_all = pd.concat(cov_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2635644-6a17-4671-911e-9324cdcff727",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cov_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec749d-518e-4e9a-af35-41efb6a1cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "dois = []\n",
    "\n",
    "for df in cov_dfs:\n",
    "    n_total += 1\n",
    "    url_col = \"url\"\n",
    "    if not url_col:\n",
    "        print(f\"Missing url column with columns\\n{df.columns}\")\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    dois.append(df.iloc[0][\"doi\"])\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)\n",
    "\n",
    "for doi in sorted(dois):\n",
    "    print(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0db813-f3d3-4b77-9090-1539bf191693",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    if \" cov\" not in col:\n",
    "        pass\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e87923-bd6f-4254-9543-67fcb19e6862",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Labelled data coverage columns (A) PANGAEA > Roelfsema et. al.\n",
    "\n",
    "Formed from the following 4 dataset \"publication series\":\n",
    "\n",
    "    https://doi.pangaea.de/10.1594/PANGAEA.891711 (CC-BY-3.0)\n",
    "    https://doi.pangaea.de/10.1594/PANGAEA.891736 (CC-BY-3.0)\n",
    "    https://doi.pangaea.de/10.1594/PANGAEA.892623 (CC-BY-3.0)\n",
    "    https://doi.pangaea.de/10.1594/PANGAEA.894801 (CC-BY-4.0)\n",
    "\n",
    "All datasets have the same 61 coverage column labels:\n",
    "\n",
    "    Acropora cov\n",
    "    Acropora cov_2\n",
    "    Acroporidae cov\n",
    "    Acroporidae cov_2\n",
    "    Acroporidae cov_3\n",
    "    Acroporidae cov_4\n",
    "    Montipora cov\n",
    "    Montipora cov_2\n",
    "    Acropora cov_3\n",
    "    Acropora cov_4\n",
    "    Acropora cov_5\n",
    "    Acropora cov_6\n",
    "    Favia cov\n",
    "    Favia cov_2\n",
    "    Coral indet cov\n",
    "    Coral indet cov_2\n",
    "    Coral indet cov_3\n",
    "    Coral indet cov_4\n",
    "    Pocilloporidae cov\n",
    "    Pocilloporidae cov_2\n",
    "    P. cylindrica cov\n",
    "    P. cylindrica cov_2\n",
    "    P. lichen cov\n",
    "    P. lichen cov_2\n",
    "    P. lobata cov\n",
    "    P. lobata cov_2\n",
    "    Coral indet cov_5\n",
    "    Coral indet cov_6\n",
    "    Coral indet cov_7\n",
    "    Coral indet cov_8\n",
    "    Alcyoniidae cov\n",
    "    Alcyoniidae cov_2\n",
    "    Gorgonia cov\n",
    "    Gorgonia cov_2\n",
    "    A. planci cov\n",
    "    Invertebrata cov\n",
    "    Soft corals oth cov\n",
    "    Soft corals oth cov_2\n",
    "    Invertebrata cov_2\n",
    "    Sand cov\n",
    "    Other cov\n",
    "    Other cov_2\n",
    "    Other cov_3\n",
    "    Other cov_4\n",
    "    Background cov\n",
    "    Algae cov\n",
    "    Benth microalgae cov\n",
    "    Corall algae cov\n",
    "    Corall algae cov_2\n",
    "    Caulerpa sp. cov\n",
    "    Chlorodesmis sp. cov\n",
    "    Cyanobact cov\n",
    "    Dictyota sp. cov\n",
    "    Epith algal matrix cov\n",
    "    Epith algal matrix cov_2\n",
    "    Lobophora cov\n",
    "    Halimeda sp. cov\n",
    "    Sargassum sp. cov\n",
    "    Padina sp. cov\n",
    "    Turbinaria sp. cov\n",
    "    Seagr cov\n",
    "\n",
    "And all have metadata columns:\n",
    "\n",
    "    image\n",
    "    url\n",
    "    latitude\n",
    "    longitude\n",
    "    Cover branch cor\n",
    "    Cover branch cor_2\n",
    "    site\n",
    "    dataset_title\n",
    "    doi\n",
    "    dataset\n",
    "    timestamp\n",
    "\n",
    "Note that not all of the labels are manually generated. Abstract for one of them:\n",
    "A subset of photoquadrats were uploaded to the CoralNet machine learning interface (https://coralnet.ucsd.edu/) and manually labelled for coral, algae or substrate type using 50 points per quadrat. Follow training of the machine, this enabled automatic annotation of all unclassified field images: the remaining field photos were uploaded to the database and 50 annotation points were overlaid on each of the images. Every point was assigned a benthic cover category from a label list automatically by the program. The resulting benthic cover data of each photo was linked to gps coordinates, saved as an ArcMap point shapefile, and projected to Universal Transverse Mercator WGS84 Zone 55 South."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9ba04-999e-43cb-9348-45b5f071fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_count_ = defaultdict(lambda: 0)\n",
    "column_examples_ = defaultdict(lambda: [])\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "dois = []\n",
    "\n",
    "first = True\n",
    "\n",
    "cov_dfs_sub = []\n",
    "\n",
    "for df in cov_dfs:\n",
    "    if \"seagr cov\" not in [c.lower() for c in df.columns]:\n",
    "        continue\n",
    "    if \"acropora cov\" not in [c.lower() for c in df.columns]:\n",
    "        continue\n",
    "    n_total += 1\n",
    "    url_col = \"url\"\n",
    "    if not url_col:\n",
    "        print(f\"Missing url column with columns\\n{df.columns}\")\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    dois.append(df.iloc[0][\"doi\"])\n",
    "    cov_dfs_sub.append(df)\n",
    "\n",
    "    for col in df.columns:\n",
    "        column_count_[col] += 1\n",
    "        column_examples_[col].append(fname)\n",
    "\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)\n",
    "    if first:\n",
    "        for c in df.columns:\n",
    "            if \" cov\" not in c:\n",
    "                print(c)\n",
    "        for c in df.columns:\n",
    "            if \" cov\" in c:\n",
    "                print(c)\n",
    "        first = False\n",
    "    last_df = df\n",
    "\n",
    "for doi in sorted(dois):\n",
    "    print(doi)\n",
    "\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" not in c:\n",
    "        print(c)\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" in c:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc6f9e-bede-4846-ab52-9fc08129d3af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count_)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count_.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    if \" cov\" not in col:\n",
    "        pass\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90ce2a-61d5-4d38-9415-9c93a2037c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = [k for k in column_count_ if \" cov\" in k]\n",
    "\n",
    "cov_df_sub = pd.concat(cov_dfs_sub)\n",
    "\n",
    "print(f\"Number of records: {len(cov_df_sub):6d}  (initial)\")\n",
    "\n",
    "cov_df_sub = cov_df_sub[~cov_df_sub[label_columns].isna().all(\"columns\")]\n",
    "\n",
    "print(\n",
    "    f\"Number of records: {len(cov_df_sub):6d}  (remove records where all label columns are NaN)\"\n",
    ")\n",
    "\n",
    "cols_for_dup = list(cov_df_sub.columns)\n",
    "cols_for_dup.remove(\"dataset\")\n",
    "cols_for_dup.remove(\"dataset_title\")\n",
    "cols_for_dup.remove(\"doi\")\n",
    "cols_for_dup.remove(\"site\")\n",
    "cols_for_dup.remove(\"image\")\n",
    "# cols_for_dup.remove(\"Method comm\")\n",
    "cov_df_sub.drop_duplicates(subset=cols_for_dup, inplace=True)\n",
    "\n",
    "print(\n",
    "    f\"Number of records: {len(cov_df_sub):6d}  (after removing duplicates across datasets)\"\n",
    ")\n",
    "print(f\"Number of uq urls: {len(cov_df_sub.drop_duplicates(subset='url')):6d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae352046-28b5-4e0f-8f0f-93ecd85af414",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_sub[cov_df_sub.duplicated(subset=\"url\")][\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade0dc3-27cb-4e2f-bff7-c4988919dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_sub[cov_df_sub.duplicated(subset=\"url\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c11ebb-91c2-425c-8f0f-9713f9a4cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_urls = cov_df_sub[cov_df_sub.duplicated(subset=\"url\")][\"url\"].unique()\n",
    "\n",
    "for url in dup_urls:\n",
    "    print(url, sum(cov_df_sub[\"url\"] == url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6916a4e-2146-4a29-ba4d-ffb6e7f06aa1",
   "metadata": {},
   "source": [
    "We found a URL, repeated 1982 times, which was not correct. The correct URLs are as per the image field:\n",
    "\n",
    "- URL: https://hs.pangaea.de/Images/Benthos/Great_Barrier_Reef/CCMR/Opal_Reef_2017-01/20170126_Opal_S_C14_289.jpg\n",
    "- Image: https://hs.pangaea.de/Images/Benthos/Great_Barrier_Reef/CCMR/Opal_Reef_2017-01/20170126_Opal_DC2-002.jpg\n",
    "\n",
    "This is now fixed by the ``fixup_repeated_urls`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c7e47-595d-41c7-9bf1-86321e8765cc",
   "metadata": {},
   "source": [
    "We also found one URL which is repeated incorrectly again. The correct URLs are as per the image field:\n",
    "\n",
    "- 1: https://hs.pangaea.de/Images/Benthos/Heron_Reef/2018/20181113_Heron_HeronBommie_-001.jpg\n",
    "- 2: https://hs.pangaea.de/Images/Benthos/Heron_Reef/2018/20181114_Heron_HarrysBommie_-001.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea206d2-9a24-4897-9443-f5d96af95cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_sub = fixup_repeated_urls(cov_df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f66ea-c8d1-47f5-8563-f44d84385eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of records: {len(cov_df_sub):6d}\")\n",
    "print(f\"Number of uq urls: {len(cov_df_sub.drop_duplicates(subset='url')):6d}\")\n",
    "\n",
    "cov_df_sub.to_csv(\n",
    "    f\"../pangaea_coverage-a_{datetime.datetime.today().strftime('%Y-%m-%d')}.csv\",\n",
    "    index=False,\n",
    ")\n",
    "display(cov_df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883904a-ceb6-44e0-9132-ab0210cc18ea",
   "metadata": {},
   "source": [
    "### Labelled data coverage columns (B) PANGAEA > Roelfsema et. al.\n",
    "\n",
    "All datasets in the publication series\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.846147 (CC-BY-3.0)\n",
    "\n",
    "Sited at Eastern Banks, Moreton Bay.\n",
    "\n",
    "Published in https://doi.org/10.1038/sdata.2015.40\n",
    "\n",
    "Note\n",
    "The 2015 data was additional data that was collected in the same manner as the other years. Although it was not used in the publication, we believed that it should be added to the data set as it expands the collection for anyone wanting to use the data in their own way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5503b2f-12b1-4e9b-8b03-fbf685ef9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_count_ = defaultdict(lambda: 0)\n",
    "column_examples_ = defaultdict(lambda: [])\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "dois = []\n",
    "\n",
    "first = True\n",
    "\n",
    "cov_dfs_sub = []\n",
    "\n",
    "for df in cov_dfs:\n",
    "    if \"seagr cov\" not in [c.lower() for c in df.columns]:\n",
    "        continue\n",
    "    if \"acropora cov\" in [c.lower() for c in df.columns]:\n",
    "        continue\n",
    "    n_total += 1\n",
    "    url_col = \"url\"\n",
    "    if not url_col:\n",
    "        print(f\"Missing url column with columns\\n{df.columns}\")\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    dois.append(df.iloc[0][\"doi\"])\n",
    "    cov_dfs_sub.append(df)\n",
    "\n",
    "    for col in df.columns:\n",
    "        column_count_[col] += 1\n",
    "        column_examples_[col].append(fname)\n",
    "\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)\n",
    "    if first:\n",
    "        for c in df.columns:\n",
    "            if \" cov\" not in c:\n",
    "                print(c)\n",
    "        for c in df.columns:\n",
    "            if \" cov\" in c:\n",
    "                print(c)\n",
    "        first = False\n",
    "    last_df = df\n",
    "\n",
    "for doi in sorted(dois):\n",
    "    print(doi)\n",
    "\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" not in c:\n",
    "        print(c)\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" in c:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43df25-752b-49b2-be33-0af0f1880450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count_)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count_.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    if \" cov\" not in col:\n",
    "        pass\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618b4c6-4389-421b-9da6-cf0a482cdc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = [k for k in column_count_ if \" cov\" in k]\n",
    "\n",
    "cov_df_sub = pd.concat(cov_dfs_sub)\n",
    "\n",
    "print(f\"Number of records: {len(cov_df_sub):6d}  (initial)\")\n",
    "\n",
    "cov_df_sub = cov_df_sub[~cov_df_sub[label_columns].isna().all(\"columns\")]\n",
    "\n",
    "print(\n",
    "    f\"Number of records: {len(cov_df_sub):6d}  (remove records where all label columns are NaN)\"\n",
    ")\n",
    "\n",
    "cols_for_dup = list(cov_df_sub.columns)\n",
    "cols_for_dup.remove(\"dataset\")\n",
    "cols_for_dup.remove(\"dataset_title\")\n",
    "cols_for_dup.remove(\"doi\")\n",
    "cols_for_dup.remove(\"site\")\n",
    "cols_for_dup.remove(\"image\")\n",
    "# cols_for_dup.remove(\"Method comm\")\n",
    "cov_df_sub.drop_duplicates(subset=cols_for_dup, inplace=True)\n",
    "\n",
    "print(\n",
    "    f\"Number of records: {len(cov_df_sub):6d}  (after removing duplicates across datasets)\"\n",
    ")\n",
    "print(f\"Number of uq urls: {len(cov_df_sub.drop_duplicates(subset='url')):6d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b52d2-8c0e-4355-8b8b-c7df97f6267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_sub[cov_df_sub.duplicated(subset=\"url\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da58c49-e98d-4e57-97cb-2d4689b44ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_sub[cov_df_sub.duplicated(subset=\"url\")].iloc[0][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ab51c-b5fb-49b7-b080-4de3298889b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_sub[\n",
    "    cov_df_sub[\"url\"]\n",
    "    == \"https://store.pangaea.de/Images/Benthos/EasternBanks/2004-07/2004-07-30_AM3_transect_023.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a198d-4ce3-451c-ba52-55b108cd6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, vals in cov_df_sub[\n",
    "    cov_df_sub[\"url\"]\n",
    "    == \"https://store.pangaea.de/Images/Benthos/EasternBanks/2004-07/2004-07-30_AM3_transect_023.jpg\"\n",
    "].iteritems():\n",
    "    if isinstance(vals.iloc[0], str):\n",
    "        if (vals == vals.iloc[0]).all():\n",
    "            continue\n",
    "    else:\n",
    "        if np.allclose(vals.iloc[0], vals, equal_nan=True):\n",
    "            continue\n",
    "\n",
    "    print(col)\n",
    "    print(vals)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee4a9b-626b-426c-9502-aed9aadc0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop this record, since labels are incompatible\n",
    "cov_df_sub = cov_df_sub[\n",
    "    cov_df_sub[\"url\"]\n",
    "    != \"https://store.pangaea.de/Images/Benthos/EasternBanks/2004-07/2004-07-30_AM3_transect_023.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1904ff-8771-47eb-841f-14746fb0ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of records: {len(cov_df_sub):6d}\")\n",
    "print(f\"Number of uq urls: {len(cov_df_sub.drop_duplicates(subset='url')):6d}\")\n",
    "\n",
    "cov_df_sub.to_csv(\n",
    "    f\"../pangaea_coverage-b_{datetime.datetime.today().strftime('%Y-%m-%d')}.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "display(cov_df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdadb68a-1ce1-47a7-9454-738d5b875f59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Labelled data coverage columns (C) Other\n",
    "\n",
    "The rest. 3 datasets in Antarctica, Arctic, and Chile.\n",
    "\n",
    "    https://doi.pangaea.de/10.1594/PANGAEA.839225 (CC-BY-3.0)\n",
    "    https://doi.pangaea.de/10.1594/PANGAEA.841459 (CC-BY-3.0)\n",
    "    https://doi.pangaea.de/10.1594/PANGAEA.897047 (CC-BY-4.0)\n",
    "    https://aslopubs.onlinelibrary.wiley.com/doi/10.1002/lno.11187\n",
    "\n",
    "They are all interested in Bryozoa, but otherwise there isn't any overlap in labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837b197-5ab0-4467-bc1e-1c6062fc044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_count_ = defaultdict(lambda: 0)\n",
    "column_examples_ = defaultdict(lambda: [])\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "dois = []\n",
    "\n",
    "first = True\n",
    "\n",
    "cov_dfs_sub = []\n",
    "\n",
    "for df in cov_dfs:\n",
    "    if \"seagr cov\" in [c.lower() for c in df.columns]:\n",
    "        continue\n",
    "    if \"acropora cov\" not in [c.lower() for c in df.columns]:\n",
    "        pass\n",
    "    n_total += 1\n",
    "    url_col = \"url\"\n",
    "    if not url_col:\n",
    "        print(f\"Missing url column with columns\\n{df.columns}\")\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    dois.append(df.iloc[0][\"doi\"])\n",
    "    print(\"{}: {}\".format(df.iloc[0][\"doi\"], len(df)))\n",
    "    cov_dfs_sub.append(df)\n",
    "\n",
    "    for col in df.columns:\n",
    "        column_count_[col] += 1\n",
    "        column_examples_[col].append(fname)\n",
    "\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)\n",
    "    if first:\n",
    "        for c in df.columns:\n",
    "            if \" cov\" not in c:\n",
    "                print(c)\n",
    "        for c in df.columns:\n",
    "            if \" cov\" in c:\n",
    "                print(c)\n",
    "        first = False\n",
    "    last_df = df\n",
    "\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" not in c:\n",
    "        print(c)\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" in c:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0e08f-8354-4946-ab80-801c3a992a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count_)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count_.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    if \" cov\" not in col:\n",
    "        pass\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7207dd3-03be-40db-abe7-472bfdeafd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cov_df_sub in cov_dfs_sub:\n",
    "    dataset = cov_df_sub.loc[0, \"dataset\"]\n",
    "    print()\n",
    "    print(dataset)\n",
    "    print()\n",
    "\n",
    "    label_columns = [k for k in cov_df_sub.columns if \" cov\" in k]\n",
    "\n",
    "    print(f\"Number of label columns: {len(label_columns)}\")\n",
    "    print(f\"Number of records: {len(cov_df_sub):6d}  (initial)\")\n",
    "\n",
    "    cov_df_sub = cov_df_sub[~cov_df_sub[label_columns].isna().all(\"columns\")]\n",
    "\n",
    "    print(\n",
    "        f\"Number of records: {len(cov_df_sub):6d}  (remove records where all label columns are NaN)\"\n",
    "    )\n",
    "\n",
    "    cols_for_dup = list(cov_df_sub.columns)\n",
    "    cols_for_dup.remove(\"dataset\")\n",
    "    cols_for_dup.remove(\"dataset_title\")\n",
    "    cols_for_dup.remove(\"doi\")\n",
    "    cols_for_dup.remove(\"site\")\n",
    "    cols_for_dup.remove(\"image\")\n",
    "    # cols_for_dup.remove(\"Method comm\")\n",
    "    cov_df_sub = cov_df_sub.drop_duplicates(subset=cols_for_dup)\n",
    "\n",
    "    print(\n",
    "        f\"Number of records: {len(cov_df_sub):6d}  (after removing duplicates across datasets)\"\n",
    "    )\n",
    "    print(f\"Number of uq urls: {len(cov_df_sub.drop_duplicates(subset='url')):6d}\")\n",
    "\n",
    "    if len(cov_df_sub) > 1000:\n",
    "        dsid = dataset.replace(\"pangaea-\", \"\")\n",
    "        cov_df_sub.to_csv(\n",
    "            f\"../pangaea_coverage-{dsid}_{datetime.datetime.today().strftime('%Y-%m-%d')}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f5dfb-97ea-40fc-b7c1-a34f2e991a9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Additional labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6685770-28b8-4c10-8ff3-163443b08ac3",
   "metadata": {},
   "source": [
    "### Redo column name tally\n",
    "\n",
    "But with URLs filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7efb3-0564-4ae9-8888-a93ec824568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) valid datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    if \" cov\" in col:\n",
    "        continue\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290acb6-eaef-404f-bd27-6aca495d1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = column_examples[\"Marine litter\"][0]\n",
    "df = pd.read_csv(os.path.join(dirname, fname))\n",
    "url_column = find_url_column(df)\n",
    "print(url_column)\n",
    "# Remove bad URLs\n",
    "df = df[df[url_column].apply(checker.is_url)]\n",
    "display(df)\n",
    "print(df.columns)\n",
    "print(df[url_column].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6f3b2-529e-4345-94c4-0da418252f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a63fd-0f75-41cc-aecb-1aa7a9382724",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = column_examples[\"Plank\"][0]\n",
    "df = pd.read_csv(os.path.join(dirname, fname))\n",
    "url_column = find_url_column(df)\n",
    "print(url_column)\n",
    "# Remove bad URLs\n",
    "df = df[df[url_column].apply(checker.is_url)]\n",
    "display(df)\n",
    "print(df.columns)\n",
    "print(df[url_column].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dcc42d-80f7-46e5-b5f1-977982abd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = column_examples[\"Squat lobster\"][0]\n",
    "df = pd.read_csv(os.path.join(dirname, fname))\n",
    "url_column = find_url_column(df)\n",
    "print(url_column)\n",
    "# Remove bad URLs\n",
    "df = df[df[url_column].apply(checker.is_url)]\n",
    "display(df)\n",
    "print(df.columns)\n",
    "print(df[url_column].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23e3aa-c841-40cb-9494-912c8726db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = column_examples[\"Hard substrat\"][0]\n",
    "df = pd.read_csv(os.path.join(dirname, fname))\n",
    "url_column = find_url_column(df)\n",
    "print(url_column)\n",
    "# Remove bad URLs\n",
    "df = df[df[url_column].apply(checker.is_url)]\n",
    "display(df)\n",
    "print(df.columns)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"doi\"].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
