{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab3f23-0a3d-4df4-abb6-20f61fe0cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pangaea_downloader.tools import checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35131f3-6be1-4db6-bd98-1517d62cf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"../query-outputs2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f2898-25da-4cea-99fa-3aa74a4f4e3e",
   "metadata": {},
   "source": [
    "## Load datasets and check distribution of column frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e38b1-f7e7-4e9e-a55f-7a9ac6adf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url_column(df):\n",
    "\n",
    "    clean_cols = [\n",
    "        col.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\").replace(\".\", \"\")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    # Ordered list of priorities\n",
    "    # Exclude url meta/ref/source which are not links to images\n",
    "    candidates = [\n",
    "        \"urlimage\",\n",
    "        \"urlraw\",\n",
    "        \"urlfile\",\n",
    "        \"url\",\n",
    "        \"urlgraphic\",\n",
    "        \"urlthumb\",\n",
    "        \"urlthumbnail\",\n",
    "        \"image\",\n",
    "        \"imagery\",\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if candidate not in clean_cols:\n",
    "            continue\n",
    "        col = df.columns[clean_cols.index(candidate)]\n",
    "        if any(df[col].apply(checker.is_url)):\n",
    "            return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e6ccb-2634-48ff-928b-54c73579c36b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_count = defaultdict(lambda: 0)\n",
    "column_examples = defaultdict(lambda: [])\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "for fname in tqdm(os.listdir(dirname)):\n",
    "    ds_id = os.path.splitext(fname)[0]\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    n_total += 1\n",
    "    if not checker.has_url_col(df):\n",
    "        continue\n",
    "    url_col = find_url_column(df)\n",
    "    if not url_col:\n",
    "        print(f\"No url column for {fname} with columns\\n{df.columns}\")\n",
    "        files_without_url.append(fname)\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    for col in df.columns:\n",
    "        col = col.lower().strip()\n",
    "        column_count[col] += 1\n",
    "        column_examples[col].append(fname)\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b763999-c70c-45be-91f5-806a7fef5f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b478a-bd3d-417f-8e88-f49ea585c812",
   "metadata": {},
   "source": [
    "### Examining columns to find out what their contents are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df384e8f-569d-4ba7-9ac5-94215cf73db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"resolution\"][0]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb89dc0-4eb3-437a-8db9-95bc4778c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "n_datasets = 0\n",
    "for fname in column_examples[\"seagr cov\"]:\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    url_column = find_url_column(df)\n",
    "    if url_column:\n",
    "        urls.append(df[url_column])\n",
    "        n_datasets += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fb907-9719-4250-9fdd-1b274641a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.concat(urls).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775e137-d8d2-4b11-959e-300c053f3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705a972-2212-47fb-862c-d27cfa8e9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up largest dataset, containing long cruises\n",
    "df = pd.read_csv(os.path.join(dirname, \"882349.csv\"))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8fb82-29d6-4537-aa36-58204ea58870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354140a-0c95-42b3-8863-8ceadea95f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(-df[\"Depth water\"], label=\"Depth water\")\n",
    "plt.plot(df[\"Elevation\"], label=\"Elevation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df[\"Depth water\"] + df[\"Elevation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf521eb-f434-47b2-bc07-db240dc34bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(column_examples[\"seagr cov\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65082d0e-9d20-451c-b9cf-3ed816067a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(column_examples[\"seagr cov\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe71a6-83f8-47c2-b290-ce879f1e3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_columns = [k for k in column_examples.keys() if k.endswith(\" cov\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268bd50-1560-4b55-8092-31c02f424dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coverage_datasets = set()\n",
    "for k in coverage_columns:\n",
    "    if k == \"ice cov\":\n",
    "        continue\n",
    "    print()\n",
    "    print(k)\n",
    "    print(len(coverage_datasets))\n",
    "    coverage_datasets.update(column_examples[k])\n",
    "    print(len(coverage_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee86b7-c394-4ead-a05d-d867fb923a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coverage_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a0dcf-7397-4165-8fe4-c087470a1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "n_datasets = 0\n",
    "for fname in coverage_datasets:\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    url_column = find_url_column(df)\n",
    "    if url_column:\n",
    "        urls.append(df[url_column])\n",
    "        n_datasets += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a29df-7717-4c8e-a356-d975a879d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.concat(urls).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7a847-1d8a-4f43-a2c8-dcd18b4c22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"ice cov\"][1]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d57b4-5410-41ac-bfd7-952101e4415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"ph\"][0]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884eaf21-56f2-4da1-8627-45b334c29369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"elevation\"][-1]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3025d87-ba7b-4aca-8828-d758da26443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, column_examples[\"doi\"][0]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dffa8b-0075-4a63-b547-c84d51e7bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirname, files_with_repeat_urls[3]))\n",
    "display(df)\n",
    "print(df.columns)\n",
    "url_column = find_url_column(df)\n",
    "print(df[url_column].iloc[0])\n",
    "print(df[\"dataset_title\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd2af0-f9b7-46dc-80ab-fcc8ab54bf45",
   "metadata": {},
   "source": [
    "## Implement dataset cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7111b-cca5-4f29-b49b-806ca744c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXONOMY_RANKS = [\n",
    "    [\"Kingdom\", \"Regnum\"],\n",
    "    [\"Phylum\", \"Division\"],\n",
    "    [\"Ordo\", \"Order\"],\n",
    "    [\"Familia\", \"Family\"],\n",
    "    [\"Genus\"],\n",
    "    [\"Species\"],\n",
    "]\n",
    "\n",
    "\n",
    "def row2taxonomy(row):\n",
    "    parts = []\n",
    "    for rank_synonyms in TAXONOMY_RANKS:\n",
    "        for col in rank_synonyms:\n",
    "            col_ = col.lower()\n",
    "            if col in row.keys() and row[col] and row[col] != \"-\":\n",
    "                parts.append(row[col])\n",
    "                break\n",
    "            elif col_ in row.keys() and row[col_] and row[col_] != \"-\":\n",
    "                parts.append(row[col_])\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return \" > \".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1931b2-0617-421f-ae7f-ed62c2b0cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_title(title):\n",
    "    \"\"\"\n",
    "    Screen dataset title.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        The title of the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Whether the dataset title is acceptable.\n",
    "    \"\"\"\n",
    "\n",
    "    title = str(title)\n",
    "\n",
    "    if title.startswith(\"Meteorological observations\"):\n",
    "        return False\n",
    "    if title.startswith(\"Sea ice conditions\"):\n",
    "        return False\n",
    "    if \"topsoil\" in title.lower():\n",
    "        return False\n",
    "    if \"core\" in title.lower():\n",
    "        # return False\n",
    "        pass\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def add_file_extension(row):\n",
    "    \"\"\"\n",
    "    Add file extension to image filename.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : dict\n",
    "        A dict record which may have fields ``\"image\"``, ``\"File format\"``, ``\"File type\"``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fname : str\n",
    "        File name with extension included.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        \"image\" not in row.keys()\n",
    "        or not row[\"image\"]\n",
    "        or not isinstance(row[\"image\"], str)\n",
    "    ):\n",
    "        return \"\"\n",
    "\n",
    "    s = row[\"image\"]\n",
    "    ext = os.path.splitext(s)[-1]\n",
    "    if (\n",
    "        ext.lower()\n",
    "        in checker.VALID_IMG_EXTENSIONS\n",
    "        + checker.INVALID_FILE_EXTENSIONS\n",
    "        + checker.COMPRESSED_FILE_EXTENSIONS\n",
    "    ):\n",
    "        return s\n",
    "\n",
    "    for col in [\"File format\", \"File type\"]:\n",
    "        if col not in row.keys():\n",
    "            continue\n",
    "        new_ext = row[col]\n",
    "        if not new_ext or not isinstance(new_ext, str):\n",
    "            continue\n",
    "        new_ext = \".\" + new_ext.strip().lstrip(\".\")\n",
    "        if ext == new_ext:\n",
    "            break\n",
    "        s += new_ext\n",
    "        break\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def datetime2timestamp(ts):\n",
    "    \"\"\"\n",
    "    Convert a datetime string to a timestamp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ts : str\n",
    "        Datetime string, in a format understood by ``dateutil.parser``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Timestamp; number of seconds since Unix epoch.\n",
    "    \"\"\"\n",
    "    if isinstance(ts, str):\n",
    "        return dateutil.parser.parse(ts).timestamp()\n",
    "    return ts\n",
    "\n",
    "\n",
    "def reformat_df(df, remove_duplicate_columns=True):\n",
    "    \"\"\"\n",
    "    Reformat/clean pangaea dataset.\n",
    "\n",
    "    Rename columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Original dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame or None\n",
    "        Cleaned dataset, or ``None`` if the dataset is invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    if (\n",
    "        \"dataset_title\" in df\n",
    "        and len(df) > 0\n",
    "        and df.iloc[0][\"dataset_title\"]\n",
    "        and not check_title(df.iloc[0][\"dataset_title\"])\n",
    "    ):\n",
    "        return None\n",
    "\n",
    "    # Make a copy of the dataframe so we can't overwrite the input\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove bad columns\n",
    "    df.drop(labels=[\"-\"], axis=\"columns\", inplace=True, errors=\"ignore\")\n",
    "    # Remove duplicately named columns\n",
    "    cols_to_drop = []\n",
    "    if remove_duplicate_columns:\n",
    "        for col in df.columns:\n",
    "            if len(col) < 2:\n",
    "                continue\n",
    "            if (\n",
    "                (col[-2] in \" _\")\n",
    "                and (col[-1] in \"123456789\")\n",
    "                and (col[:-2] in df.columns)\n",
    "            ):\n",
    "                cols_to_drop.append(col)\n",
    "        df.drop(labels=cols_to_drop, axis=\"columns\", inplace=True)\n",
    "\n",
    "    # Find the correct URL column, and drop other columns containing \"url\"\n",
    "    cols_to_drop = []\n",
    "    mapping = {}\n",
    "    col_url = find_url_column(df)\n",
    "    mapping[col_url] = \"url\"\n",
    "    for col in df.columns:\n",
    "        if col != col_url and \"url\" in col.lower():\n",
    "            cols_to_drop.append(col)\n",
    "\n",
    "    # Search for matches to canonical columns.\n",
    "    # Each entry in desired_columns is a key, value pair where the key\n",
    "    # is the output column name, and the value is a list of search names\n",
    "    # in order of priority. The first match will be kept and others discarded.\n",
    "    desired_columns = {\n",
    "        \"dataset\": [\"ds_id\", \"dataset\", \"Campaign\", \"campaign\"],\n",
    "        \"site\": [\"Event\", \"event\", \"Site\", \"site\", \"deployment\"],\n",
    "        \"image\": [\"image\", \"filename\"],\n",
    "        \"timestamp\": [\"Timestamp\"],\n",
    "        \"latitude\": [\"Latitude\", \"latitude\", \"lat\", \"latitude+\"],\n",
    "        \"longitude\": [\"Longitude\", \"longitude\", \"lon\", \"long\", \"longitude+\"],\n",
    "        \"x_pos\": [],\n",
    "        \"y_pos\": [],\n",
    "        \"altitude\": [\"altitude\", \"height\"],\n",
    "        \"depth\": [\n",
    "            \"depthwater\",\n",
    "            \"bathydepth\",\n",
    "            \"bathymetry\",\n",
    "            \"bathy\",\n",
    "            \"depth\",\n",
    "            \"elevation\",\n",
    "        ],\n",
    "        \"backscatter\": [],\n",
    "        \"temperature\": [\"temperature\", \"temp\"],\n",
    "        \"salinity\": [\"salinity\", \"sal\"],\n",
    "        \"chlorophyll\": [],\n",
    "        \"acidity\": [\"pH\"],\n",
    "        \"doi\": [\"DOI\", \"doi\"],\n",
    "    }\n",
    "    # Remove non-alphanumeric padding characters, including spaces, from actual column names\n",
    "    raw_cols = list(df.columns)\n",
    "    clean_cols = [\n",
    "        col.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\").replace(\".\", \"\")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    # Map to lower case\n",
    "    lower_cols = [col.lower() for col in clean_cols]\n",
    "\n",
    "    # Search for matching column names\n",
    "    for canon, searches in desired_columns.items():\n",
    "        found = False\n",
    "\n",
    "        # Check for case-sensitive, non-alphanumeric, match\n",
    "        for search in searches:\n",
    "            if search not in raw_cols:\n",
    "                continue\n",
    "            col = search\n",
    "            if not found:\n",
    "                found = True\n",
    "                mapping[col] = canon\n",
    "                if col != canon and canon in df.columns:\n",
    "                    cols_to_drop.append(canon)\n",
    "            elif col not in mapping and col not in cols_to_drop:\n",
    "                cols_to_drop.append(col)\n",
    "\n",
    "        # Check for case-sensitive match\n",
    "        for search in searches:\n",
    "            if search not in clean_cols:\n",
    "                continue\n",
    "            col = df.columns[clean_cols.index(search)]\n",
    "            if not found:\n",
    "                found = True\n",
    "                mapping[col] = canon\n",
    "                if col != canon and canon in df.columns:\n",
    "                    cols_to_drop.append(canon)\n",
    "            elif col not in mapping and col not in cols_to_drop:\n",
    "                cols_to_drop.append(col)\n",
    "\n",
    "        # Check for case-insensitive match\n",
    "        for search in searches:\n",
    "            if search.lower() not in lower_cols:\n",
    "                continue\n",
    "            col = df.columns[lower_cols.index(search.lower())]\n",
    "            if not found:\n",
    "                found = True\n",
    "                mapping[col] = canon\n",
    "                if col != canon and canon in df.columns:\n",
    "                    cols_to_drop.append(canon)\n",
    "            elif col not in mapping and col not in cols_to_drop:\n",
    "                cols_to_drop.append(col)\n",
    "\n",
    "    # Remove superfluous columns\n",
    "    df.drop(labels=cols_to_drop, axis=\"columns\", inplace=True)\n",
    "    # Rename columns to canonical names\n",
    "    df.rename(columns=mapping, inplace=True, errors=\"raise\")\n",
    "\n",
    "    # Add file extension to image\n",
    "    df[\"image\"] = df.apply(add_file_extension, axis=1)\n",
    "    if \"timestamp\" not in df.columns and \"Date/Time\" in df.columns:\n",
    "        df[\"timestamp\"] = df[\"Date/Time\"].apply(datetime2timestamp)\n",
    "\n",
    "    if any([c in clean_cols for c in [\"Kingdom\", \"Phylum\", \"Genus\"]]):\n",
    "        df[\"taxonomy\"] = df.apply(row2taxonomy, axis=1)\n",
    "        df.drop(\n",
    "            labels=[x for syn in TAXONOMY_RANKS for x in syn],\n",
    "            axis=\"columns\",\n",
    "            inplace=True,\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "\n",
    "    cols_to_drop = [\n",
    "        \"File format\",\n",
    "        \"File type\",\n",
    "        \"File size\",\n",
    "        \"Date/Time\",\n",
    "        \"Date/time end\",\n",
    "    ]\n",
    "    df.drop(labels=cols_to_drop, axis=\"columns\", inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21314a94-a03c-4317-87fb-0578e3f04381",
   "metadata": {},
   "source": [
    "## Load data with dataset cleaning functions applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed7c35-614c-40b8-a192-e3ef24b984cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_count = defaultdict(lambda: 0)\n",
    "column_examples = defaultdict(lambda: [])\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "dfs = []\n",
    "dfs_fnames = []\n",
    "\n",
    "for fname in tqdm(os.listdir(dirname)):\n",
    "    ds_id = os.path.splitext(fname)[0]\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    n_total += 1\n",
    "    if not checker.has_url_col(df):\n",
    "        continue\n",
    "    # print(df.columns)\n",
    "    url_col = find_url_column(df)\n",
    "    if not url_col:\n",
    "        # print(f\"No url column for {fname} with columns\\n{df.columns}\")\n",
    "        files_without_url.append(fname)\n",
    "        continue\n",
    "    df[\"ds_id\"] = f\"pangaea-{ds_id}\"\n",
    "    df = reformat_df(df)\n",
    "    if df is None:\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    dfs.append(df)\n",
    "    dfs_fnames.append(fname)\n",
    "    for col in df.columns:\n",
    "        column_count[col] += 1\n",
    "        column_examples[col].append(fname)\n",
    "    # print(df.columns)\n",
    "    url_col = \"url\"\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e5487-685f-49ec-aac3-2af68b942a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) valid datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315634da-ccc1-45f5-9eff-c298c518e765",
   "metadata": {},
   "source": [
    "### Merge datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666b6ba-7513-41fc-b504-76b40d6e9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cols = {\n",
    "    \"dataset\",\n",
    "    \"site\",\n",
    "    \"url\",\n",
    "    \"image\",\n",
    "    \"timestamp\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"x_pos\",\n",
    "    \"y_pos\",\n",
    "    \"altitude\",\n",
    "    \"depth\",\n",
    "    \"backscatter\",\n",
    "    \"temperature\",\n",
    "    \"salinity\",\n",
    "    \"chlorophyll\",\n",
    "    \"acidity\",\n",
    "}\n",
    "\n",
    "df_all = pd.concat([df[df.columns.intersection(select_cols)] for df in dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa83a9-05fd-4161-be11-3f6369f8a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eddfd9-5541-4681-8da5-8ec4b8950eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all records\n",
    "df_all.to_csv(\n",
    "    f\"../pangaea_{datetime.datetime.today().strftime('%Y-%m-%d')}.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082806f5-8aeb-4b97-8e39-c5749b3b7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only valid URLs\n",
    "df_all = df_all[df_all[\"url\"].apply(checker.is_url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4a895-2086-49c4-8c67-1b1930b76c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ffef8-ff55-48f6-b588-ffbdf036145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:4])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f7f88-d2a5-43b5-9190-5af4a6c058e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8dd81e-1126-4bb8-a2bd-2c66c6785da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = unique_url_bases[0]\n",
    "df_all[df_all[\"url\"].str.startswith(url_base)].iloc[[0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8106df-d060-43ec-b91e-3a23fa7ee1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c7b28-64fb-4950-8082-94f884f329ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows have lat & lon\n",
    "sum(~df_all[\"latitude\"].isna() & ~df_all[\"longitude\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a511931-45ab-46a6-a37c-337ee9324fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837620f-29ba-421b-93ae-1ff1dda78b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate URLs\n",
    "df_all = df_all.drop_duplicates(subset=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98d486-cf02-4797-98e4-dafaed33a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37dcd1b-345c-4a9a-8c0a-916913f296c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows have lat & lon\n",
    "sum(~df_all[\"latitude\"].isna() & ~df_all[\"longitude\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456fb65-4762-4d27-9585-abf6d23cead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693c9c0-96d4-4006-b3cc-41bfb959a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_image = df_all[\"url\"].apply(\n",
    "    lambda x: checker.has_img_extension(x.rstrip(\"/\"))\n",
    ") | df_all[\"image\"].apply(lambda x: checker.has_img_extension(x.rstrip(\"/\")))\n",
    "df_all = df_all[is_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb88409-50aa-463c-824f-c74d76180271",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d8862-607e-48f7-b8ff-dfa4fa54be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(~df_all[\"latitude\"].isna() & ~df_all[\"longitude\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807fed2-52b0-4f7c-97c9-9833679fd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f48d22-7fdd-4d45-80e0-7413d2568df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:4])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46de5eb-5a71-4ce2-9f12-139a482c8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a0418-aa84-41a9-94b9-2780d7eba123",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i + 1, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5b1f9-9148-4858-a1ed-d5a5d225f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subdomain(url):\n",
    "    blacklist = [\n",
    "        \"https://doi.org/10.1594/PANGAEA\",\n",
    "        \"http://epic.awi.de/\",\n",
    "        \"https://epic.awi.de/\",\n",
    "        \"http://hdl.handle.net/10013/\",\n",
    "        \"http://library.ucsd.edu/dc/object/\",\n",
    "        \"https://hs.pangaea.de/Maps/\",\n",
    "        \"https://hs.pangaea.de/Movies/\",\n",
    "        \"https://hs.pangaea.de/Projects/\",\n",
    "        \"https://hs.pangaea.de/bathy/\",\n",
    "        \"https://hs.pangaea.de/fishsounder/\",\n",
    "        \"https://hs.pangaea.de/mag/\",\n",
    "        \"https://hs.pangaea.de/model/\",\n",
    "        \"https://hs.pangaea.de/nav/\",\n",
    "        \"https://hs.pangaea.de/palaoa/\",\n",
    "        \"https://hs.pangaea.de/para/\",\n",
    "        \"https://hs.pangaea.de/reflec/\",\n",
    "        \"https://hs.pangaea.de/sat/\",\n",
    "        \"https://prr.osu.edu/collection/object/\",\n",
    "        \"https://store.pangaea.de/Projects/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/Publications/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/software/\",\n",
    "        \"https://www.ngdc.noaa.gov/geosamples/\",\n",
    "    ]\n",
    "    for entry in blacklist:\n",
    "        if url.startswith(entry):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d54c9-6f69-4229-b5ec-ad62d4876dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad URLs based on their subdomain\n",
    "df_all = df_all[df_all[\"url\"].apply(check_subdomain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd747c-1bdb-43eb-a5bd-af83bfb7494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797d5fb-929a-48f1-a55c-985bfdcbc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:5])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15212b59-e4c5-4d23-ba2c-7b9cc597400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6b436-d7ee-4748-b609-869bff5bd7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i + 1, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce63a9-4e1b-44a5-b95f-5d88298ce878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d14f94-f85e-4a74-b589-1e99ed53facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.search(\n",
    "    \"(?<![A-Za-z])map(?![A-Za-z])\",\n",
    "    \"https://hs.pangaea.de/Images/ROV/M/M114/GeoB19346-1/data_publish/config/divemap/Dive360_map.jpg\",\n",
    "):\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b03137-b9f3-4e4c-b46d-1a2c62430112",
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.search(\n",
    "    \"(?<![A-Za-z])map(?![A-Za-z])\",\n",
    "    \"https://hs.pangaea.de/Images/ROV/M/M114/GeoB19346-1/data_publish/config/divemap/Dive360mapy.jpg\",\n",
    "):\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd73bc0-7f1f-4aec-bca0-b4ee872a9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subdomain(url):\n",
    "    blacklist = [\n",
    "        \"https://doi.org/10.1594/PANGAEA\",\n",
    "        \"http://epic.awi.de/\",\n",
    "        \"https://epic.awi.de/\",\n",
    "        \"http://hdl.handle.net/10013/\",\n",
    "        \"http://library.ucsd.edu/dc/object/\",\n",
    "        \"https://app.geosamples.org/uploads/UHM\",\n",
    "        \"https://hs.pangaea.de/Images/Linescan\",\n",
    "        \"https://hs.pangaea.de/Maps/\",\n",
    "        \"https://hs.pangaea.de//Maps\",\n",
    "        \"https://hs.pangaea.de/Movies/\",\n",
    "        \"https://hs.pangaea.de/Projects/\",\n",
    "        \"https://hs.pangaea.de/bathy/\",\n",
    "        \"https://hs.pangaea.de/fishsounder/\",\n",
    "        \"https://hs.pangaea.de/mag/\",\n",
    "        \"https://hs.pangaea.de/model/\",\n",
    "        \"https://hs.pangaea.de/nav/\",\n",
    "        \"https://hs.pangaea.de/palaoa/\",\n",
    "        \"https://hs.pangaea.de/pasata/\",\n",
    "        \"https://hs.pangaea.de/para/\",\n",
    "        \"https://hs.pangaea.de/polar\",\n",
    "        \"https://hs.pangaea.de/reflec/\",\n",
    "        \"https://hs.pangaea.de/sat/\",\n",
    "        \"https://prr.osu.edu/collection/object/\",\n",
    "        \"https://store.pangaea.de/Projects/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/Publications/\",  # Not all bad, but mostly\n",
    "        \"https://store.pangaea.de/software/\",\n",
    "        \"https://www.ngdc.noaa.gov/geosamples/\",\n",
    "        \"https://hs.pangaea.de/Images/Airphoto\",\n",
    "        # \"https://hs.pangaea.de/Images/Cores\",  # Some of these are okay\n",
    "        \"https://hs.pangaea.de/Images/Documentation\",\n",
    "        \"https://hs.pangaea.de/Images/Maps\",\n",
    "        \"https://hs.pangaea.de/Images/MMT/\",\n",
    "        \"https://hs.pangaea.de/Images/Plankton\",\n",
    "        # The GeoB19346-1 dataset contains .bmp images of the ROV's sonar\n",
    "        \"https://hs.pangaea.de/Images/ROV/M/M114/GeoB19346-1/data_publish/data/sonar/\",\n",
    "        \"https://hs.pangaea.de/Images/Satellite\",\n",
    "        \"https://hs.pangaea.de/Images/SeaIce\",\n",
    "        \"https://hs.pangaea.de/Images/Water\",\n",
    "        \"https://store.pangaea.de/Images/Airphoto\",\n",
    "        \"https://store.pangaea.de/Images/Documentation\",\n",
    "    ]\n",
    "    banned_words = [\"map\", \"divemap\", \"dredge_photos\", \"dredgephotograph\"]\n",
    "    for entry in blacklist:\n",
    "        if url.startswith(entry):\n",
    "            return False\n",
    "    for word in banned_words:\n",
    "        if re.search(\"(?<![A-Za-z])\" + word + \"(?![A-Za-z])\", url.lower()):\n",
    "            return False\n",
    "    if re.search(\"(?<![a-z])core(?![a-rty])\", url.lower()) and \"SUR\" not in url:\n",
    "        # Images of cores must contain \"SURFACE\", or the shorthand \"SUR\"\n",
    "        # We only keep the ones with surface in uppercase, because those\n",
    "        # experiments are in-situ surface photos, whereas lower case are not.\n",
    "        return False\n",
    "    if \"not_available\" in url:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7fb7d-f80a-4788-bb22-102419e32be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad URLs based on their subdomain (again), and remove maps\n",
    "df_all = df_all[df_all[\"url\"].apply(check_subdomain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f68e32-c94a-46a5-b1ec-330b2fe182a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39795a9-49a1-4d54-b3fa-0532c403b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_url_bases = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: \"/\".join(x.split(\"/\")[:5])).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db689e-1288-405e-bfb6-89e58a91ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_url_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f6870-57fd-4884-84c9-34ef5df34e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url_base in enumerate(unique_url_bases):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.startswith(url_base)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), base {}\".format(\n",
    "            i + 1, len(unique_url_bases), len(sdf), url_base\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[499])\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[4999])\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7a1c8-920f-4d26-823a-da683f8e9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all[\"url\"].apply(lambda x: \"mosaic\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037953bb-6831-4f53-806d-254b313604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop mosaic images\n",
    "df_all = df_all[~df_all[\"url\"].apply(lambda x: \"mosaic\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da8200-c0f8-4aa8-baf2-8d95d0a52077",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_extensions = sorted(\n",
    "    df_all[\"url\"].apply(lambda x: os.path.splitext(x)[1]).unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9df6bc-5a67-4ffa-bc55-de1d855ff971",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a71797e-8da2-497b-ba20-dc4101d18c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ext in enumerate(unique_extensions):\n",
    "    print()\n",
    "    sdf = df_all[df_all[\"url\"].str.endswith(ext)]\n",
    "    print(\n",
    "        \"{:3d}/{} ({:7d} URLs), extension {}\".format(\n",
    "            i + 1, len(unique_extensions), len(sdf), ext\n",
    "        )\n",
    "    )\n",
    "    print(sdf[\"url\"].iloc[0])\n",
    "    if len(sdf) > 2:\n",
    "        print(sdf[\"url\"].iloc[1])\n",
    "    if len(sdf) > 4:\n",
    "        print(sdf[\"url\"].iloc[len(sdf) // 2])\n",
    "    if len(sdf) > 12:\n",
    "        print(sdf[\"url\"].iloc[9])\n",
    "    if len(sdf) > 102:\n",
    "        print(sdf[\"url\"].iloc[99])\n",
    "    if len(sdf) > 1002:\n",
    "        print(sdf[\"url\"].iloc[499])\n",
    "        print(sdf[\"url\"].iloc[999])\n",
    "    if len(sdf) > 10002:\n",
    "        print(sdf[\"url\"].iloc[4999])\n",
    "        print(sdf[\"url\"].iloc[9999])\n",
    "    if len(sdf) > 3:\n",
    "        print(sdf[\"url\"].iloc[-2])\n",
    "    if len(sdf) > 1:\n",
    "        print(sdf[\"url\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08727d-7895-41a2-8cfc-c1eecd4641e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60186b43-b1ee-4d82-b208-65ccc793244a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save unlabelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9bde3-cbda-402d-ba37-122b0507b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\n",
    "    f\"../pangaea_{datetime.datetime.today().strftime('%Y-%m-%d')}_filtered.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9f49f-fa72-4175-bd09-231db4f45cf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Labelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a26fca-abd4-4cad-8575-dc66b1842ae9",
   "metadata": {},
   "source": [
    "## Percent Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10ff34d-71fa-461b-88b0-be5c1145032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_dfs = []\n",
    "for fname in tqdm(coverage_datasets):\n",
    "    ds_id = os.path.splitext(fname)[0]\n",
    "    df = pd.read_csv(os.path.join(dirname, fname))\n",
    "    if not checker.has_url_col(df):\n",
    "        continue\n",
    "    # print(df.columns)\n",
    "    url_col = find_url_column(df)\n",
    "    if not url_col:\n",
    "        print(f\"No url column for {fname} with columns\\n{df.columns}\")\n",
    "        continue\n",
    "    df[\"ds_id\"] = f\"pangaea-{ds_id}\"\n",
    "    df = reformat_df(df, remove_duplicate_columns=False)\n",
    "    if df is None or len(df) == 0:\n",
    "        continue\n",
    "    df = df[~df[\"url\"].isna()]\n",
    "    df = df[df[\"url\"].apply(check_subdomain)]\n",
    "    is_image = df[\"url\"].apply(lambda x: checker.has_img_extension(x.rstrip(\"/\"))) | df[\n",
    "        \"image\"\n",
    "    ].apply(lambda x: checker.has_img_extension(x.rstrip(\"/\")))\n",
    "    df = df[is_image]\n",
    "    if df is None or len(df) == 0:\n",
    "        continue\n",
    "    cov_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca4fa07-cc87-4480-8e0f-2060168447ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_all = pd.concat(cov_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2635644-6a17-4671-911e-9324cdcff727",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cov_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec749d-518e-4e9a-af35-41efb6a1cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "dois = []\n",
    "\n",
    "for df in cov_dfs:\n",
    "    n_total += 1\n",
    "    url_col = \"url\"\n",
    "    if not url_col:\n",
    "        print(f\"Missing url column with columns\\n{df.columns}\")\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    dois.append(df.iloc[0][\"doi\"])\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)\n",
    "\n",
    "for doi in sorted(dois):\n",
    "    print(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0db813-f3d3-4b77-9090-1539bf191693",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    if \" cov\" not in col:\n",
    "        pass\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e87923-bd6f-4254-9543-67fcb19e6862",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Labelled data coverage columns (A) PANGAEA > Roelfsema et. al.\n",
    "\n",
    "Formed from the following 4 dataset \"publication series\":\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.891711 (CC-BY-3.0)\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.891736 (CC-BY-3.0)\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.892623 (CC-BY-3.0)\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.894801 (CC-BY-4.0)\n",
    "\n",
    "All datasets have the same 61 coverage column labels:\n",
    "```\n",
    "    Acropora cov\n",
    "    Acropora cov_2\n",
    "    Acroporidae cov\n",
    "    Acroporidae cov_2\n",
    "    Acroporidae cov_3\n",
    "    Acroporidae cov_4\n",
    "    Montipora cov\n",
    "    Montipora cov_2\n",
    "    Acropora cov_3\n",
    "    Acropora cov_4\n",
    "    Acropora cov_5\n",
    "    Acropora cov_6\n",
    "    Favia cov\n",
    "    Favia cov_2\n",
    "    Coral indet cov\n",
    "    Coral indet cov_2\n",
    "    Coral indet cov_3\n",
    "    Coral indet cov_4\n",
    "    Pocilloporidae cov\n",
    "    Pocilloporidae cov_2\n",
    "    P. cylindrica cov\n",
    "    P. cylindrica cov_2\n",
    "    P. lichen cov\n",
    "    P. lichen cov_2\n",
    "    P. lobata cov\n",
    "    P. lobata cov_2\n",
    "    Coral indet cov_5\n",
    "    Coral indet cov_6\n",
    "    Coral indet cov_7\n",
    "    Coral indet cov_8\n",
    "    Alcyoniidae cov\n",
    "    Alcyoniidae cov_2\n",
    "    Gorgonia cov\n",
    "    Gorgonia cov_2\n",
    "    A. planci cov\n",
    "    Invertebrata cov\n",
    "    Soft corals oth cov\n",
    "    Soft corals oth cov_2\n",
    "    Invertebrata cov_2\n",
    "    Sand cov\n",
    "    Other cov\n",
    "    Other cov_2\n",
    "    Other cov_3\n",
    "    Other cov_4\n",
    "    Background cov\n",
    "    Algae cov\n",
    "    Benth microalgae cov\n",
    "    Corall algae cov\n",
    "    Corall algae cov_2\n",
    "    Caulerpa sp. cov\n",
    "    Chlorodesmis sp. cov\n",
    "    Cyanobact cov\n",
    "    Dictyota sp. cov\n",
    "    Epith algal matrix cov\n",
    "    Epith algal matrix cov_2\n",
    "    Lobophora cov\n",
    "    Halimeda sp. cov\n",
    "    Sargassum sp. cov\n",
    "    Padina sp. cov\n",
    "    Turbinaria sp. cov\n",
    "    Seagr cov\n",
    "```\n",
    "And all have metadata columns:\n",
    "```\n",
    "    image\n",
    "    url\n",
    "    latitude\n",
    "    longitude\n",
    "    Cover branch cor\n",
    "    Cover branch cor_2\n",
    "    site\n",
    "    dataset_title\n",
    "    doi\n",
    "    dataset\n",
    "    timestamp\n",
    "```\n",
    "\n",
    "Note that not all of the labels are manually generated. Abstract for one of them:\n",
    "A subset of photoquadrats were uploaded to the CoralNet machine learning interface (https://coralnet.ucsd.edu/) and manually labelled for coral, algae or substrate type using 50 points per quadrat. Follow training of the machine, this enabled automatic annotation of all unclassified field images: the remaining field photos were uploaded to the database and 50 annotation points were overlaid on each of the images. Every point was assigned a benthic cover category from a label list automatically by the program. The resulting benthic cover data of each photo was linked to gps coordinates, saved as an ArcMap point shapefile, and projected to Universal Transverse Mercator WGS84 Zone 55 South."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9ba04-999e-43cb-9348-45b5f071fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "dois = []\n",
    "\n",
    "first = True\n",
    "\n",
    "cov_dfs_sub = []\n",
    "\n",
    "for df in cov_dfs:\n",
    "    if \"seagr cov\" not in [c.lower() for c in df.columns]:\n",
    "        continue\n",
    "    if \"acropora cov\" not in [c.lower() for c in df.columns]:\n",
    "        continue\n",
    "    n_total += 1\n",
    "    url_col = \"url\"\n",
    "    if not url_col:\n",
    "        print(f\"Missing url column with columns\\n{df.columns}\")\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    dois.append(df.iloc[0][\"doi\"])\n",
    "    cov_dfs_sub.append(df)\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)\n",
    "    if first:\n",
    "        for c in df.columns:\n",
    "            if \" cov\" not in c:\n",
    "                print(c)\n",
    "        for c in df.columns:\n",
    "            if \" cov\" in c:\n",
    "                print(c)\n",
    "        first = False\n",
    "    last_df = df\n",
    "\n",
    "for doi in sorted(dois):\n",
    "    print(doi)\n",
    "\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" not in c:\n",
    "        print(c)\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" in c:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc6f9e-bede-4846-ab52-9fc08129d3af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    if \" cov\" not in col:\n",
    "        pass\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90ce2a-61d5-4d38-9415-9c93a2037c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_sub = pd.concat(cov_dfs_sub)\n",
    "\n",
    "cov_df_sub.to_csv(\n",
    "    f\"../pangaea_coverage-a_{datetime.datetime.today().strftime('%Y-%m-%d')}.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "display(cov_df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883904a-ceb6-44e0-9132-ab0210cc18ea",
   "metadata": {},
   "source": [
    "### Labelled data coverage columns (B) PANGAEA > Roelfsema et. al.\n",
    "\n",
    "All datasets in the publication series\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.846147 (CC-BY-3.0)\n",
    "\n",
    "Sited at Eastern Banks, Moreton Bay.\n",
    "\n",
    "Published in https://doi.org/10.1038/sdata.2015.40\n",
    "\n",
    "Note\n",
    "The 2015 data was additional data that was collected in the same manner as the other years. Although it was not used in the publication, we believed that it should be added to the data set as it expands the collection for anyone wanting to use the data in their own way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5503b2f-12b1-4e9b-8b03-fbf685ef9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "dois = []\n",
    "\n",
    "first = True\n",
    "\n",
    "cov_dfs_sub = []\n",
    "\n",
    "for df in cov_dfs:\n",
    "    if \"seagr cov\" not in [c.lower() for c in df.columns]:\n",
    "        continue\n",
    "    if \"acropora cov\" in [c.lower() for c in df.columns]:\n",
    "        continue\n",
    "    n_total += 1\n",
    "    url_col = \"url\"\n",
    "    if not url_col:\n",
    "        print(f\"Missing url column with columns\\n{df.columns}\")\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    dois.append(df.iloc[0][\"doi\"])\n",
    "    cov_dfs_sub.append(df)\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)\n",
    "    if first:\n",
    "        for c in df.columns:\n",
    "            if \" cov\" not in c:\n",
    "                print(c)\n",
    "        for c in df.columns:\n",
    "            if \" cov\" in c:\n",
    "                print(c)\n",
    "        first = False\n",
    "    last_df = df\n",
    "\n",
    "for doi in sorted(dois):\n",
    "    print(doi)\n",
    "\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" not in c:\n",
    "        print(c)\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" in c:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43df25-752b-49b2-be33-0af0f1880450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    if \" cov\" not in col:\n",
    "        pass\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1904ff-8771-47eb-841f-14746fb0ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_sub = pd.concat(cov_dfs_sub)\n",
    "\n",
    "cov_df_sub.to_csv(\n",
    "    f\"../pangaea_coverage-b_{datetime.datetime.today().strftime('%Y-%m-%d')}.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "display(cov_df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdadb68a-1ce1-47a7-9454-738d5b875f59",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Labelled data coverage columns (C) Other\n",
    "\n",
    "The rest. 3 datasets in Antarctica, Arctic, and Chile.\n",
    "\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.839225 (CC-BY-3.0)\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.841459 (CC-BY-3.0)\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.897047 (CC-BY-4.0)\n",
    "    https://aslopubs.onlinelibrary.wiley.com/doi/10.1002/lno.11187\n",
    "\n",
    "They are all interested in Bryozoa, but otherwise there isn't any overlap in labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837b197-5ab0-4467-bc1e-1c6062fc044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "dois = []\n",
    "\n",
    "first = True\n",
    "\n",
    "cov_dfs_sub = []\n",
    "\n",
    "for df in cov_dfs:\n",
    "    if \"seagr cov\" in [c.lower() for c in df.columns]:\n",
    "        continue\n",
    "    if \"acropora cov\" not in [c.lower() for c in df.columns]:\n",
    "        pass\n",
    "    n_total += 1\n",
    "    url_col = \"url\"\n",
    "    if not url_col:\n",
    "        print(f\"Missing url column with columns\\n{df.columns}\")\n",
    "        continue\n",
    "    n_valid += 1\n",
    "    dois.append(df.iloc[0][\"doi\"])\n",
    "    cov_dfs_sub.append(df)\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)\n",
    "    if first:\n",
    "        for c in df.columns:\n",
    "            if \" cov\" not in c:\n",
    "                print(c)\n",
    "        for c in df.columns:\n",
    "            if \" cov\" in c:\n",
    "                print(c)\n",
    "        first = False\n",
    "    last_df = df\n",
    "\n",
    "for doi in sorted(dois):\n",
    "    print(doi)\n",
    "\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" not in c:\n",
    "        print(c)\n",
    "for c in last_df.columns:\n",
    "    if \" cov\" in c:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0e08f-8354-4946-ab80-801c3a992a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(\n",
    "    f\"Of which {len(files_with_repeat_urls)} have repeated URLs (possibly multiple annotations)\"\n",
    ")\n",
    "print()\n",
    "print(f\"There are {len(column_count)} unique column names:\")\n",
    "print()\n",
    "\n",
    "for col, count in dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ").items():\n",
    "    if \" cov\" not in col:\n",
    "        pass\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88f684-7320-4bd4-bd81-b1fc1cb1e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_sub = pd.concat(cov_dfs_sub)\n",
    "\n",
    "cov_df_sub.to_csv(\n",
    "    f\"../pangaea_coverage-c_{datetime.datetime.today().strftime('%Y-%m-%d')}.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "display(cov_df_sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
