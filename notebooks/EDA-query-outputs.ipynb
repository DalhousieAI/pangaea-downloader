{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from pangaea_downloader import checker, utilz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files from downloads directory\n",
    "TEST_DIR = \"../query-outputs/\"\n",
    "files = os.listdir(TEST_DIR)\n",
    "df_list = [pd.read_csv(os.path.join(TEST_DIR, f)) for f in files]\n",
    "\n",
    "# List of valid image file extensions\n",
    "valid_img_extensions = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Count images in each file\n",
    "- Count values in URL column\n",
    "- Number of valid URLs\n",
    "- Number of invalid URLs\n",
    "- URLs with image file extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_counts = []\n",
    "for i, (file, df) in enumerate(zip(files, df_list)):\n",
    "    # Count only the first url column\n",
    "    col = utilz.get_url_cols(df)[0]\n",
    "    # Count stuff\n",
    "    n_rows = len(df)\n",
    "    count = df[col].count()\n",
    "    valid_url = df[col].apply(checker.is_url).sum()\n",
    "    invalid_url = count - valid_url\n",
    "    valid_img_ext = df[col].apply(checker.is_img_url).sum()\n",
    "    missing = df[col].isna().sum()\n",
    "    # Keep record of counts\n",
    "    img_counts.append(\n",
    "        {\n",
    "            \"file\": file,\n",
    "            \"column\": col,\n",
    "            \"n_rows\": n_rows,\n",
    "            \"count\": count,\n",
    "            \"valid_url\": valid_url,\n",
    "            \"invalid_url\": invalid_url,\n",
    "            \"valid_img_ext\": valid_img_ext,\n",
    "            \"missing\": missing,\n",
    "        }\n",
    "    )\n",
    "# Make a dataframe\n",
    "img_counts = pd.DataFrame(img_counts)\n",
    "\n",
    "# Show resuts\n",
    "print(f\"Raw image count in all files: {img_counts['count'].sum()}\")\n",
    "print(f\"Total number of valid urls: {img_counts['valid_url'].sum()}\")\n",
    "print(f\"Total number of valid image urls: {img_counts['valid_img_ext'].sum()}\")\n",
    "img_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Campaigns\n",
    "- How many campaigns (Why 33 campaigns for 290+ files?)\n",
    "- Distribution of images across campaigns\n",
    "- How many sites\n",
    "- Distribution of images across sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Check if each file has only one campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets with more than one campaign (should be empty)\n",
    "a = [df[\"Campaign\"].unique() for df in df_list if df[\"Campaign\"].nunique() > 1]\n",
    "if not len(a) > 0:\n",
    "    print(\"Each files has one associated campaign.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Number of datasets per campaign\n",
    "Many of the datasets are from the same campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.DataFrame(\n",
    "    {\n",
    "        \"file\": files,\n",
    "        \"campaign\": [df[\"Campaign\"].unique()[0] for df in df_list],\n",
    "        \"total_nans\": [df.isna().sum().sum() for df in df_list],\n",
    "        \"nan_percent\": [round(df.isna().sum().sum() / df.size, 4) for df in df_list],\n",
    "    }\n",
    ")\n",
    "datasets.loc[35:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total datasets: {datasets['campaign'].count()}\")\n",
    "print(f\"Total number of campaigns in all files: {datasets['campaign'].nunique()}\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Number of datasets from each campaign\")\n",
    "sns.countplot(data=datasets, y=\"campaign\", edgecolor=\"black\", linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Number of images per campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camps = {campaign: 0 for campaign in datasets[\"campaign\"].unique()}\n",
    "for df in df_list:\n",
    "    campaign = df[\"Campaign\"].unique()[0]\n",
    "    img_cols = utilz.get_url_cols(df)\n",
    "    if len(img_cols) > 0:\n",
    "        img_col = img_cols[0]\n",
    "        camps[campaign] += df[img_col].count()\n",
    "camps = pd.Series(camps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Number of images per campaign\")\n",
    "sns.barplot(y=camps.index, x=camps.values, edgecolor=\"black\", linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze missing values\n",
    "- Raw total missing values\n",
    "- How many missing values in mandatory columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 How many datasets have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "fig.suptitle(\"Missing values across datasets\")\n",
    "ax[0].hist(datasets[\"total_nans\"], bins=20, edgecolor=\"black\", linewidth=1)\n",
    "ax[0].set_xlabel(\"Number of missing values\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "# ax[0].set_yscale(\"log\")\n",
    "ax[1].hist(datasets[\"nan_percent\"], bins=20, edgecolor=\"black\", linewidth=1)\n",
    "ax[1].set_xlabel(\"Percentage of missing values\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "# ax[1].set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see most datasets have close to 0 missing values. There are a few datasets with 5,000 or close to 40,000 missing values. The percentage plot also shows a similar picture. Most datasets have below 10% missing values. While a few have 20-25% missing. Percentages are calcualted by dividing the total number of missing values of a dataset and dividing by the size (rowsXcols) of then dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Detailed breakdown of missing values\n",
    "Let us now examine each dataset and check which columns have how many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (df, file) in enumerate(zip(df_list, files)):\n",
    "    nans_per_column = df.isna().sum()\n",
    "    total_nans = nans_per_column.sum()\n",
    "    if total_nans > 0:\n",
    "        print(f\"[{i}][{file}] Total {total_nans} null values in dataframe.\")\n",
    "        display(nans_per_column[nans_per_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial distribution of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all data\n",
    "all_dfs = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Explore Latitude, Longitude metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range\n",
    "print(f\" Latitude range: {all_dfs.Latitude.min()} to {all_dfs.Latitude.max()}\")\n",
    "print(f\"Longitude range: {all_dfs.Longitude.min()} to {all_dfs.Longitude.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts\n",
    "all_dfs[[\"Latitude\", \"Longitude\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "all_dfs[[\"Latitude\", \"Longitude\"]].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=all_dfs, x=\"Longitude\", y=\"Latitude\", kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_basemap(df: pd.DataFrame, full_map=True):\n",
    "    llcrnrlat = df.Latitude.min() if not full_map else -90\n",
    "    urcrnrlat = df.Latitude.max() if not full_map else 90\n",
    "    llcrnrlon = df.Longitude.min() if not full_map else -180\n",
    "    urcrnrlon = df.Longitude.max() if not full_map else 180\n",
    "\n",
    "    m = Basemap(\n",
    "        projection=\"mill\",\n",
    "        resolution=\"c\",\n",
    "        llcrnrlat=llcrnrlat,\n",
    "        urcrnrlat=urcrnrlat,\n",
    "        llcrnrlon=llcrnrlon,\n",
    "        urcrnrlon=urcrnrlon,\n",
    "    )\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare map\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "m = make_basemap(all_dfs, full_map=True)\n",
    "# m.drawcoastlines()\n",
    "m.drawlsmask(land_color=\"grey\", ocean_color=\"white\", lakes=True)\n",
    "# m.etopo()\n",
    "# m.bluemarble()\n",
    "# m.shadedrelief()\n",
    "m.drawparallels(np.arange(-90, 90, step=10), labels=[1, 0, 0, 0])\n",
    "m.drawmeridians(np.arange(-180, 180, step=30), labels=[0, 0, 0, 1])\n",
    "\n",
    "# Plot data\n",
    "lon_x = all_dfs[\"Longitude\"].to_list()\n",
    "lat_y = all_dfs[\"Latitude\"].to_list()\n",
    "\n",
    "m.scatter(lon_x, lat_y, latlon=True, alpha=0.25, s=20, c=\"red\", marker=\"o\")\n",
    "\n",
    "plt.title(\"Spatial distribution of Pangaea dataset images\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot Sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Take a sample of image urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample each file/dataset's url column\n",
    "sample_imgs = []\n",
    "for i, (file, df) in enumerate(zip(files, df_list)):\n",
    "    # Take a sample from the first url column\n",
    "    col = utilz.get_url_cols(df)[0]\n",
    "    sample = df[col].sample().iloc[0]\n",
    "    # Check if it is string and is valid url\n",
    "    if (\n",
    "        isinstance(sample, str)\n",
    "        and checker.is_url(sample)\n",
    "        and (sample.lower().endswith(valid_img_extensions))\n",
    "    ):\n",
    "        sample_imgs.append(sample)\n",
    "\n",
    "# Keep a subset of samples\n",
    "sample_imgs = np.random.choice(sample_imgs, size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Retrieve sampled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add function to utilz.py\n",
    "def img_from_url(url: str, verbose=False) -> np.array:\n",
    "    \"\"\"Take an image url and return retrieved image array.\"\"\"\n",
    "    success = False\n",
    "    while not success:\n",
    "        resp = requests.get(url, stream=True)\n",
    "        print(f\"status code: {resp.status_code}\") if verbose else 0\n",
    "        success = True if (resp.status_code == 200) else False\n",
    "        if success:\n",
    "            arr = np.asarray(bytearray(resp.content), dtype=np.uint8)\n",
    "            img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Plot sampled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 4\n",
    "nrows = int(len(sample_imgs) / ncols)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, nrows * 4))\n",
    "for ax, url in zip(axes.flat, sample_imgs):\n",
    "    # Retrieve image from url\n",
    "    print(f\"Retrieving: {url} ...\")\n",
    "    img = img_from_url(url, verbose=True)\n",
    "    # Plot image\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Shape: {img.shape}\")\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor(\"w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (ws)",
   "language": "python",
   "name": "ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
