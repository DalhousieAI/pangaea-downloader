{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# sys.path.append(\"..\")\n",
    "# from pangaea_downloader import utilz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load downloaded data\n",
    "Load the data we downloaded from Pangaea. Presumably these should datasets of \"*seabed photographs*\" which was the search query that resulted in the downloaded data. \n",
    "\n",
    "**\\# Mandatory Columns**<br>\n",
    "- image URL\n",
    "- longitude\n",
    "- latitude\n",
    "- campaign name (ID)\n",
    "- deployment/site/dive\n",
    "\n",
    "**\\# Optional Columns**<br>\n",
    "  - depth (optional)\n",
    "  - altitude (optional)\n",
    "  - timestamp (optional)\n",
    "  - web/media source (optional)\n",
    "  - name of provider (optional)\n",
    "  - salinity (optional)\n",
    "  - chlorophyll concentration (optional)\n",
    "  - temperature (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = \"../query-outputs\"\n",
    "# List of files in directory\n",
    "files = os.listdir(TEST_DIR)\n",
    "N_FILES = len(files)\n",
    "print(f\"[INFO] Total {N_FILES} files in directory.\")\n",
    "# Load data\n",
    "df_list = [pd.read_csv(os.path.join(TEST_DIR, f)) for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_list[16]\n",
    "print(f\"[INFO] {df.shape[0]} rows X {df.shape[1]} columns\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values in each column of dataset\n",
    "nans_per_column = df.isna().sum()\n",
    "# Total null values in dataset\n",
    "total_nans = nans_per_column.sum()\n",
    "print(f\"[INFO] Total {total_nans} null values in dataframe.\")\n",
    "# Showing only the columns with null values\n",
    "nans_per_column[nans_per_column > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Although there are quite a few null values in the 'Course' and 'Speed' column, the dataset isn't missing values in the mandatory columns such as Image URL, Latitude, Longitude, campaign name or site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Number of Images (with valid urls)\n",
    "- Since the name for the image URL column varies with each file/dataset we have to first identify which column(s) have URLs.\n",
    "\n",
    "- We should also check if the values in the URL columns have valid URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_cols(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Take a Pandas DataFrame and return a list of URL columns.\"\"\"\n",
    "    return [col for col in df.columns if (\"url\" in col.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "get_url_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_url(string: str) -> bool:\n",
    "    \"\"\"src: https://stackoverflow.com/questions/7160737/how-to-validate-a-url-in-python-malformed-or-not\"\"\"\n",
    "    regex = re.compile(\n",
    "        r\"^(?:http|ftp)s?://\"  # http:// or https://\n",
    "        r\"(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|\"  # domain...\n",
    "        r\"localhost|\"  # localhost...\n",
    "        r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\"  # ...or ip\n",
    "        r\"(?::\\d+)?\"  # optional port\n",
    "        r\"(?:/?|[/?]\\S+)$\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "    return re.match(regex, string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(is_url(\"http://www..com\"))  # False\n",
    "print(is_url(\"http://www.google.com\"))  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of images in sample dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image URL columns\n",
    "url_cols = get_url_cols(df)\n",
    "for col in url_cols:\n",
    "    # Pick a random value of that column\n",
    "    i = np.random.randint(0, len(df))\n",
    "    sample_url = df[col][i]\n",
    "    # Check if the URL is valid\n",
    "    if is_url(df[col][0]):\n",
    "        print(f\"{col} : {df[col].count()}\")\n",
    "    else:\n",
    "        print(f\"{col} value not url! {sample_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Number of Campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Campaign\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Number of Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Site\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Plot sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_from_bytes(resp):\n",
    "    arr = np.asarray(bytearray(resp.content), dtype=np.uint8)\n",
    "    img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_img(df, verbose=False):\n",
    "    # Random sample\n",
    "    idx = np.random.randint(0, len(df))\n",
    "    # Select the first url column\n",
    "    col = get_url_cols(df)[0]\n",
    "    url = df[col][idx]\n",
    "    print(f\"Index: {idx}, Column: '{col}', URL: '{url}'\") if verbose else 0\n",
    "    # Check if URL is valids\n",
    "    if not is_url(url):\n",
    "        print(f\"[ERROR] Invalid URL: '{url}'\")\n",
    "        return\n",
    "    # Fetch image if valid URL\n",
    "    resp = requests.get(url)\n",
    "    while resp.status_code != 200:\n",
    "        url = df[col][idx]\n",
    "        # Fetch image\n",
    "        resp = requests.get(url)\n",
    "        print(\"Response status code:\", resp.status_code) if verbose else 0\n",
    "        img = img_from_bytes(resp)\n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_img(df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check all dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (df, file) in enumerate(zip(df_list, files)):\n",
    "    nans_per_column = df.isna().sum()\n",
    "    total_nans = nans_per_column.sum()\n",
    "    if total_nans > 0:\n",
    "        print(f\"[{i}][{file}] Total {total_nans} null values in dataframe.\")\n",
    "        display(nans_per_column[nans_per_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue:** Many files are missing Latitude and Longitude values: \n",
    "- 873995.csv, 873996.csv, 873997.csv, 873998.csv, 873999.csv, 874000.csv, 874001.csv, 874002.csv, 875071.csv, 875073.csv, 875080.csv, 875084.csv, 878001.csv, 878003.csv, 878004.csv, 878006.csv, 878007.csv, 878008.csv, 878009.csv, 878013.csv, 878014.csv, 878016.csv, 878019.csv, 894732.csv, 914155.csv, 914192.csv, 918924.csv, 928814.csv\n",
    "    \n",
    "**Issue:** Files missing image URL values:\n",
    "- 914212.csv, 918925.csv, 919836.csv, 928814.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Number of Images (with valid urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "valid_cols = []\n",
    "invalid_cols = []\n",
    "for i, (df, file) in enumerate(zip(df_list, files)):\n",
    "    # print(f\"[{i}][{file}]\")\n",
    "    # Get the image URL columns\n",
    "    url_cols = get_url_cols(df)\n",
    "    for col in url_cols:\n",
    "        # Pick a random value of that column\n",
    "        i = np.random.randint(0, len(df))\n",
    "        sample_url = df[col][i]\n",
    "        # Check if the URL is valid\n",
    "        if is_url(df[col][0]):\n",
    "            print(f\"[{i}] [{file}]\")\n",
    "            print(f\"\\t{col} : {df[col].count()}\")\n",
    "            d[file] = df[col].count()\n",
    "            valid_cols.append(col)\n",
    "        else:\n",
    "            # print(f\"\\tColumn name: '{col}' has invalid url value: '{sample_url}' at index: {i}\")\n",
    "            invalid_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(d).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. URL columns with valid URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique image URL columns names (valid):\")\n",
    "set(valid_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Various names of image URL column**:\n",
    "- URL\n",
    "- URL file\n",
    "- URL image\n",
    "- URL raw (also URL thumb: lower res version)\n",
    "- URL ref\n",
    "- URL source\n",
    "\n",
    "**Unexpected:**\n",
    "- URL movie (839384.csv, 839386.csv, 839387.csv, 839388.csv, 839389.csv, 839390.csv, 839391.csv, 839392.csv, 839393.csv, 839394.csv, 839395.csv, 839396.csv, 839397.csv, 839398.csv, 839399.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. URL columns with invalid URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique image URL columns names (invalid):\")\n",
    "set(invalid_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'IMAGE', 'IMAGE (Size)', 'Image' these columns were also being identified as URL columns but their values are not valid URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaigns = []\n",
    "n_campaigns = []\n",
    "for i, (df, file) in enumerate(zip(df_list, files)):\n",
    "    name = df[\"Campaign\"].unique()\n",
    "    n = df[\"Campaign\"].nunique()\n",
    "    # Add to list of campaigns\n",
    "    campaigns.extend(name)\n",
    "    n_campaigns.append(n)\n",
    "\n",
    "# Check if all files have a campaign column\n",
    "print(\"All files have campaign:\", (len(campaigns) == N_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only unique entries\n",
    "campaigns = set(campaigns)\n",
    "len(campaigns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Total number of Campaigns in all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_campaigns = set([df[\"Campaign\"].unique()[0] for df in df_list])\n",
    "print(\"Total number of campaigns:\", len(unique_campaigns), end=\"\\n\\n\")\n",
    "print(unique_campaigns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Total number of Sites in all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sites = set([df[\"Site\"].unique()[0] for df in df_list])\n",
    "print(\"Total number of sites:\", len(unique_sites), end=\"\\n\\n\")\n",
    "print(unique_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 How many images per campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camps = {campaign: 0 for campaign in unique_campaigns}\n",
    "for df in df_list:\n",
    "    campaign = df[\"Campaign\"].unique()[0]\n",
    "    img_cols = get_url_cols(df)\n",
    "    if len(img_cols) > 0:\n",
    "        img_col = img_cols[0]\n",
    "        camps[campaign] += df[img_col].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camps = pd.Series(camps).sort_values(ascending=False)\n",
    "camps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "plt.title(\"Number of images per campaign\")\n",
    "sns.barplot(x=camps.index, y=camps.values)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Plot random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (df, file) in enumerate(zip(df_list, files)):\n",
    "    plot_sample_img(df)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset metadata table\n",
    "This table will contain data about each dataset/file. Each row of will contain information such as the number of nans in that dataset, the number of images, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset metadata table\n",
    "datasets = pd.DataFrame(\n",
    "    {\n",
    "        \"filename\": files,\n",
    "        \"n_rows\": [len(df) for df in df_list],\n",
    "        \"size\": [df.size for df in df_list],  # size = rows x columns\n",
    "        \"total_nans\": [df.isna().sum().sum() for df in df_list],\n",
    "    }\n",
    ")\n",
    "# Calculate the percentage of null values in each dataframe\n",
    "datasets[\"percent_nans\"] = (datasets[\"total_nans\"] / datasets[\"size\"] * 100).round(2)\n",
    "\n",
    "print(\"Number of datasets downloaded:\", len(datasets))\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total datapoints in all datasets combined: {datasets['n_rows'].sum()}\")\n",
    "print(f\"Total null values in all datasets combined: {datasets['total_nans'].sum()}\")\n",
    "print(\n",
    "    f\"% of null values in all datasets combined: {(datasets['total_nans'].sum()/datasets['size'].sum()).round(4)}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing Files with null values\n",
    "nan_datasets = datasets[datasets[\"total_nans\"] > 0]\n",
    "print(\"Number of files with null values:\", len(nan_datasets))\n",
    "nan_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.total_nans.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "ax.hist(datasets[\"total_nans\"], bins=20)\n",
    "ax.set_title(\"Total null values in each dataframe\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xlabel(\"Number of null values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "ax.hist(nan_datasets[\"percent_nans\"], bins=20)\n",
    "ax.set_title(\"% of null values in each dataframe\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xlabel(\"% of null values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the dataset with the highest number of null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset with max null values\n",
    "idx = datasets[\"total_nans\"].argmax()\n",
    "sample = df_list[idx]\n",
    "print(\"Total NaNs in sample:\", sample.isna().sum().sum())\n",
    "sample.isna().sum()[sample.isna().any()].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Although it has the highest number of null values, it is not missing data in any of the columns we care about, such as Image URL, Latitude, Longitude, campaign name or site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (ws)",
   "language": "python",
   "name": "ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
