{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download datasets using `PanQuery` search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import pangaeapy\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from pangaea_downloader import utilz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Search Pangaea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure output directory exists\n",
    "out_dir = \"../query-outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function exceeds max 500 search result limit previously faced\n",
    "def search_pangaea(query=\"seabed photographs\", n_results=999):\n",
    "    offset = 0\n",
    "    results = []\n",
    "    while True:\n",
    "        pq = pangaeapy.PanQuery(query=query, limit=n_results, offset=offset)\n",
    "        results.extend(pq.result)\n",
    "        offset += len(pq.result)\n",
    "        if len(results) >= pq.totalcount:\n",
    "            break\n",
    "    # Sanity check\n",
    "    assert len(results) == pq.totalcount\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run search\n",
    "results = search_pangaea(query=\"seabed photographs\", n_results=999)\n",
    "print(\"Number of results returned:\", len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze results\n",
    "Before fetching and processing each of the result datasets, we will first try to analyze them without loading them into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Check if there are any duplicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DOIs for each result\n",
    "result_dois = [result[\"URI\"] for result in results]\n",
    "# Number of dois in result should equal number of unique dois in result\n",
    "if len(result_dois) == len(set(result_dois)):\n",
    "    print(\"NO DUPLICATES!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Detect the type of each result dataset\n",
    "- The `size` of the dataset can be extracted from the `result['html']` attribute. \n",
    "- We can determine the type of the dataset (parent, child, video, paginated) from the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_type(size: str) -> str:\n",
    "    \"\"\"Identify the dataset type from the size description string.\"\"\"\n",
    "    if \"bytes\" in size:\n",
    "        return \"Video\"\n",
    "    elif \"unknown\" == size:\n",
    "        return \"Paginated\"\n",
    "    elif \"datasets\" in size:\n",
    "        return \"Parent\"\n",
    "    elif \"data points\" in size:\n",
    "        return \"Child\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "ds_type(utilz.get_result_info(results[0])[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Make a dataframe describing each of the search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for i, result in enumerate(results):\n",
    "    citation, url, size, is_parent = utilz.get_result_info(result)\n",
    "    datasets.append(\n",
    "        {\n",
    "            \"doi\": result[\"URI\"],\n",
    "            \"citation\": citation,\n",
    "            \"size\": size,\n",
    "            \"is_parent\": is_parent,\n",
    "        }\n",
    "    )\n",
    "datasets = pd.DataFrame(datasets)\n",
    "datasets[\"type\"] = datasets[\"size\"].apply(ds_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show preview\n",
    "datasets.loc[270:280, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process result datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Process and examine 1 sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample\n",
    "idx = 34\n",
    "# Fetch sample\n",
    "ds = pangaeapy.PanDataSet(results[idx][\"URI\"])\n",
    "print(ds.citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Process all search result datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_downloads = 0\n",
    "result_data = []\n",
    "for i, result in enumerate(results):\n",
    "    # Extract result information\n",
    "    citation, url, size, is_parent = utilz.get_result_info(result)\n",
    "    print(f\"[{i+1}] Loading dataset: '{citation}'\")\n",
    "\n",
    "    # ------------- ASSESS DATASET TYPE ------------- #\n",
    "    df = None\n",
    "    typ = ds_type(size)\n",
    "\n",
    "    # Video dataset (ignore)\n",
    "    if typ == \"Video\":\n",
    "        print(\"\\t[WARNING] VIDEO dataset. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Paginated images (scrape urls and metadata)\n",
    "    elif typ == \"Paginated\":\n",
    "        df = utilz.scrape_images(url)\n",
    "\n",
    "    # Parent dataset (fetch child datasets)\n",
    "    elif typ == \"Parent\":\n",
    "        df = utilz.fetch_child_datasets(url)\n",
    "\n",
    "    # Tabular dataset (fetch and save)\n",
    "    elif typ == \"Child\":\n",
    "        df = utilz.fetch_dataset(url)\n",
    "\n",
    "    # ----------------- SAVE TO FILE ----------------- #\n",
    "    if df is None:\n",
    "        continue\n",
    "    else:\n",
    "        result_data.append(\n",
    "            {\n",
    "                \"doi\": result[\"URI\"],\n",
    "                \"citation\": citation,\n",
    "                \"size\": size,\n",
    "                \"is_parent\": is_parent,\n",
    "                \"missing_values\": df.isna().sum().sum(),\n",
    "            }\n",
    "        )\n",
    "        n_downloads += 1\n",
    "print(f\"COMPLETE! Total files processed: {n_downloads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = pd.DataFrame(result_data)\n",
    "result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check if all saved files have desired image url column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files in output directory\n",
    "files = os.listdir(out_dir)\n",
    "dfs = [pd.read_csv(os.path.join(out_dir, file)) for file in files]\n",
    "# Check if they have the desired column\n",
    "if all([utilz.has_url_col(df) for df in dfs]):\n",
    "    print(\"All ddownloaded files have URL column\")\n",
    "else:\n",
    "    print(\"Some files are missing URL column!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (ws)",
   "language": "python",
   "name": "ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
