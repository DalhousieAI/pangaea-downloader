{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from pangaea_downloader.tools import datasets, eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../query-outputs/\"\n",
    "files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "df_list = [pd.read_csv(os.path.join(data_dir, f), low_memory=False) for f in files]\n",
    "\n",
    "assert len(files) == len(\n",
    "    df_list\n",
    "), f\"Number of files in directory '{data_dir}' does not match number of dataframes loaded.\"\n",
    "\n",
    "print(f\"Total {len(df_list)} files loaded.\")\n",
    "sorted_dfs = sorted(df_list, key=lambda df: len(df), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if these labeled datasets have already been downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_labeled_datasets = [\n",
    "    \"https://doi.pangaea.de/10.1594/PANGAEA.846264\",\n",
    "    \"https://doi.pangaea.de/10.1594/PANGAEA.846142\",\n",
    "    \"https://doi.pangaea.de/10.1594/PANGAEA.846143\",\n",
    "    \"https://doi.pangaea.de/10.1594/PANGAEA.846144\",\n",
    "    \"https://doi.pangaea.de/10.1594/PANGAEA.846146\",\n",
    "    \"https://doi.pangaea.de/10.1594/PANGAEA.846185\",\n",
    "    \"https://doi.pangaea.de/10.1594/PANGAEA.846186\",\n",
    "    \"https://doi.pangaea.de/10.1594/PANGAEA.846266\",\n",
    "    \"https://doi.pangaea.de/10.1594/PANGAEA.867188\",\n",
    "]\n",
    "known_labeled_dsids = [url.split(\".\")[-1] for url in known_labeled_datasets]\n",
    "for i, (lab_ds_id, lab_ds) in enumerate(\n",
    "    zip(known_labeled_dsids, known_labeled_datasets)\n",
    "):\n",
    "    print(i + 1, f\"Dataset ID: '{lab_ds_id}', URL: {lab_ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert the doi/urls to the same format before comparing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_from_doi(doi: str) -> str:\n",
    "    # Already in desired format\n",
    "    if \".pangaea.de\" in doi:\n",
    "        return doi\n",
    "    # Convert to desired format\n",
    "    start, end = doi.split(\".org\")\n",
    "    full = start + \".pangaea.de\" + end\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_list[0].doi.iloc[0]\n",
    "print(\"DOI:\", test)\n",
    "print(\"URL:\", url_from_doi(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively convert to standard form and match the doi of each dataset with that of the known datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = dict()\n",
    "results = dict()\n",
    "datasets_cheked = []\n",
    "for i, df in enumerate(df_list):\n",
    "    try:\n",
    "        doi = df.doi.dropna().iloc[0]\n",
    "    except AttributeError as a:\n",
    "        errors[i] = a\n",
    "        doi = df.DOI.dropna().iloc[0]\n",
    "    finally:\n",
    "        # Convert dois to same format\n",
    "        ds_id = doi.split(\".\")[-1]\n",
    "    # Compare urls\n",
    "    for kdsid in known_labeled_dsids:\n",
    "        if ds_id == kdsid:\n",
    "            results[kdsid] = True\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for i, (lab_ds_id, lab_ds) in enumerate(\n",
    "    zip(known_labeled_dsids, known_labeled_datasets)\n",
    "):\n",
    "    for df in sorted_dfs:\n",
    "        try:\n",
    "            doi = df.doi.dropna().iloc[0]\n",
    "        except AttributeError:\n",
    "            doi = df.DOI.dropna().iloc[0]\n",
    "        finally:\n",
    "            ds_id = doi.split(\".\")[-1]\n",
    "        if ds_id == lab_ds_id:\n",
    "            results[lab_ds] = True\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify labeled datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated process for finding datasets with label columns in the format `species_cov` or `species cov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = dict()\n",
    "labelled_datasets = dict()\n",
    "labelled_dataset_idxs = []\n",
    "for i, df in enumerate(sorted_dfs):\n",
    "    try:  # Extract info\n",
    "        title = df.dataset_title.iloc[0]\n",
    "        doi = df.doi.iloc[0]\n",
    "    except AttributeError as a:\n",
    "        errors[i] = a\n",
    "        title = df.Dataset.iloc[0]\n",
    "        doi = df.DOI.iloc[0]\n",
    "    finally:\n",
    "        # Track labeled datasets with label columns in the format: `species_cov` or `species cov`\n",
    "        label_cols = [\n",
    "            col\n",
    "            for col in df.columns\n",
    "            # Inclusion criteria\n",
    "            if (\"cov\" in col.lower())\n",
    "            # Exclusion criteria\n",
    "            and (col != \"Cov\")  # Covariance\n",
    "            and (col != \"Cov std e\")  # Coverage standard error\n",
    "            and (col.lower() != \"coverage\")  # Time coverage\n",
    "            and (col.lower() != \"recov time\")  # Recover time\n",
    "            and not (\"recovery\" in col.lower())  # Recovery time\n",
    "            and not (\"ice cov\" in col.lower())  # Ice cover (sea surface not seafloor)\n",
    "            and not (\"canopy cover\" in col.lower())\n",
    "            and (col.lower() != \"ipc-cov\")\n",
    "            and (col.lower() != \"cov std dev\")\n",
    "            and (col.lower() != \"recov std dev\")\n",
    "        ]\n",
    "        if doi == \"https://doi.org/10.1594/PANGAEA.884805\":\n",
    "            continue\n",
    "\n",
    "        # Show data\n",
    "        if len(label_cols) > 0:\n",
    "            url_col = datasets.get_url_col(df)\n",
    "            n_images = len(df[url_col].dropna())\n",
    "\n",
    "            print(f\"[{str(i).zfill(4)}] {title}\")\n",
    "            print(\n",
    "                f\"N images: {n_images} (col: '{url_col}'); N label columns: {len(label_cols)}. DOI: {doi}\"\n",
    "            )\n",
    "            print(\"Label columns:\", label_cols)\n",
    "            labelled_datasets[doi] = [n_images, title, label_cols, url_col]\n",
    "            labelled_dataset_idxs.append(i)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"[DEBUG] N column name mismatches: {len(errors)}, ({round(len(errors)/len(sorted_dfs), 2)}%)\"\n",
    ")\n",
    "print(f\"[INFO] N labelled datasets: {len(labelled_datasets)}\")\n",
    "print(f\"[INFO] N labelled images: {sum([v[0] for v in labelled_datasets.values()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify ice coverage datasets and check if they also have biota label columns. If not they can be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "ice_cov_ds = []\n",
    "for df in sorted_dfs:\n",
    "    try:\n",
    "        doi = df.doi.iloc[0]\n",
    "    except AttributeError:\n",
    "        doi = df.DOI.iloc[0]\n",
    "\n",
    "    ice_cov_cols = [col for col in df.columns if (\"ice cov\" in col.lower())]\n",
    "    if len(ice_cov_cols) > 0:\n",
    "        print(f\"[{i}] {doi} : {df.columns}\")\n",
    "        ice_cov_ds.append(doi)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"doi\": labelled_datasets.keys(),\n",
    "            \"n_images\": [v[0] for v in labelled_datasets.values()],\n",
    "            \"dataset_title\": [v[1] for v in labelled_datasets.values()],\n",
    "            \"url_col\": [v[3] for v in labelled_datasets.values()],\n",
    "            \"label_cols\": [v[2] for v in labelled_datasets.values()],\n",
    "        }\n",
    "    )\n",
    "    .sort_values(by=\"n_images\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(f\"Total {results.n_images.sum()} labelled images\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel(\"../pangaea-labelled-datasets.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Process\n",
    "- Analyze column names: We begin we analyzing the dataset columns.\n",
    "- We also look the column descriptions on the dataset webpage be clicking the doi link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = dict()\n",
    "labaled_datasets = dict()\n",
    "for i, df in enumerate(sorted_dfs):\n",
    "    try:  # Extract info\n",
    "        title = df.dataset_title.iloc[0]\n",
    "        doi = df.doi.iloc[0]\n",
    "    except AttributeError:\n",
    "        title = df.Dataset.iloc[0]\n",
    "        doi = df.DOI.iloc[0]\n",
    "    # Show info\n",
    "    suffix = \">>>\" if i in labelled_dataset_idxs else \"\"\n",
    "    print(f\"{suffix}[{i}] {title}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Row: {df.shape[0]}; Columns: {df.shape[1]}. DOI: {doi}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (ws)",
   "language": "python",
   "name": "ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
