{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab3f23-0a3d-4df4-abb6-20f61fe0cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from typing import Union\n",
    "\n",
    "import dateutil.parser\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from benthicnet.io import sanitize_filename, sanitize_filename_series\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from pangaea_downloader.tools import checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from this directory\n",
    "dirname = \"../query-outputs_2022-01-01\"\n",
    "# Pangaea benthic image dataset file with filtered dataset IDs\n",
    "pangaea_file = \"../full-dataset/pangaea_2022-01-24_filtered.csv\"\n",
    "pangaea_df = pd.read_csv(pangaea_file)\n",
    "ds_ids = pangaea_df.dataset.unique()\n",
    "print(f\"Total {len(ds_ids)} datasets to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f2898-25da-4cea-99fa-3aa74a4f4e3e",
   "metadata": {},
   "source": [
    "## 1. Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_url(ds_id: Union[str, int]) -> str:\n",
    "    \"\"\"Return dataset URL given six digit dataset ID.\"\"\"\n",
    "    if isinstance(ds_id, int):\n",
    "        ds_id = str(ds_id)\n",
    "    if ds_id.startswith(\"pangaea\"):\n",
    "        ds_id = ds_id.split(\"-\")[-1]\n",
    "    if ds_id.endswith(\".csv\"):\n",
    "        ds_id = ds_id.split(\".csv\")[-2]\n",
    "    return f\"https://doi.pangaea.de/10.1594/PANGAEA.{ds_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e38b1-f7e7-4e9e-a55f-7a9ac6adf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url_column(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Find and return column with image URLs.\"\"\"\n",
    "    # Standardize column names\n",
    "    clean_cols = [\n",
    "        col.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\").replace(\".\", \"\")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    # Ordered list of priorities\n",
    "    # Exclude url meta/ref/source which are not links to images\n",
    "    candidates = [\n",
    "        \"urlimage\",\n",
    "        \"urlraw\",\n",
    "        \"urlfile\",\n",
    "        \"url\",\n",
    "        \"urlgraphic\",\n",
    "        \"urlthumb\",\n",
    "        \"urlthumbnail\",\n",
    "        \"image\",\n",
    "        \"imagery\",\n",
    "    ]\n",
    "    # Find and return the first match\n",
    "    for candidate in candidates:\n",
    "        if candidate not in clean_cols:\n",
    "            continue\n",
    "        col = df.columns[clean_cols.index(candidate)]\n",
    "        if any(df[col].apply(checker.is_url)):\n",
    "            return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e6ccb-2634-48ff-928b-54c73579c36b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Column name value counts\n",
    "column_count = defaultdict(lambda: 0)\n",
    "# Key = column name : value = dataframes with that column\n",
    "column_examples = defaultdict(lambda: [])\n",
    "\n",
    "# Files with URL issues\n",
    "files_without_url = []\n",
    "files_with_repeat_urls = []\n",
    "\n",
    "# Counts\n",
    "n_total = 0\n",
    "n_valid = 0\n",
    "\n",
    "verbose = False\n",
    "\n",
    "for dataset_name in tqdm(ds_ids):\n",
    "    # Load dataset\n",
    "    ds_id = dataset_name.split(\"-\")[1]\n",
    "    fname = f\"{ds_id}.csv\"\n",
    "    f_path = os.path.join(dirname, fname)\n",
    "    df = pd.read_csv(f_path, low_memory=False)\n",
    "    n_total += 1\n",
    "\n",
    "    # Any column names with URL or Image?\n",
    "    if not checker.has_url_col(df):\n",
    "        continue\n",
    "    # Extract the column name\n",
    "    url_col = find_url_column(df)\n",
    "\n",
    "    # No URL column found\n",
    "    if not url_col:\n",
    "        if verbose:\n",
    "            print(f\"No url column for {fname} with columns\\n{df.columns}\")\n",
    "        files_without_url.append(fname)\n",
    "        continue\n",
    "\n",
    "    # URL column found!\n",
    "    n_valid += 1\n",
    "    for col in df.columns:\n",
    "        col = col.lower().strip()\n",
    "        column_count[col] += 1\n",
    "        column_examples[col].append(fname)\n",
    "    subdf = df[df[url_col] != \"\"]\n",
    "    if len(subdf) != len(subdf.drop_duplicates(subset=url_col)):\n",
    "        files_with_repeat_urls.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b763999-c70c-45be-91f5-806a7fef5f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"There are {n_valid} valid (of {n_total}) total datasets\")\n",
    "print(f\"Of which {len(files_with_repeat_urls)} have repeated URLs\", end=\"\")\n",
    "print(\" (possibly multiple annotations)\\n\")\n",
    "print(f\"There are {len(column_count)} unique column names:\\n\")\n",
    "\n",
    "# Sort by value in descending order\n",
    "sorted_column_count = dict(\n",
    "    sorted(column_count.items(), key=lambda item: item[1], reverse=True)\n",
    ")\n",
    "for col, count in sorted_column_count.items():\n",
    "    c = col + \" \"\n",
    "    print(f\"{c:.<35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b478a-bd3d-417f-8e88-f49ea585c812",
   "metadata": {},
   "source": [
    "## 2. Examine each of the columns of interest\n",
    "- Depth water\n",
    "- Bathy depth\n",
    "- Depth bot & depth top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find depth columns\n",
    "for col in column_examples:\n",
    "    if \"depth\" in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68aed3",
   "metadata": {},
   "source": [
    "### 2.1 Depth water\n",
    "**Observations:**\n",
    "- ***Depth water*** values in ALL datasets are positive.\n",
    "- Therefore it is reasonable to assume that ***depth water*** represents the absolute distance of the camera vehicle below mean sea level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_near_zero(value, tolerance=0.5) -> bool:\n",
    "    \"\"\"Check if the input value is close to zero within a specified tolerance range.\"\"\"\n",
    "    lb = 0 - tolerance\n",
    "    ub = 0 + tolerance\n",
    "    if lb <= value <= ub:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92694f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to find\n",
    "key = \"depth water\"\n",
    "\n",
    "val_exception = {}\n",
    "for i, file in enumerate(column_examples[key]):\n",
    "    df = pd.read_csv(os.path.join(dirname, file), low_memory=False)\n",
    "    url_column = find_url_column(df)\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    # Extract info\n",
    "    mean = df[key].mean()\n",
    "    sd = df[key].std()\n",
    "    min_ = df[key].min()\n",
    "    max_ = df[key].max()\n",
    "    url = get_dataset_url(file)\n",
    "    # Check for start and end at 0 altitude/depth\n",
    "    start, end = df[key].iloc[0], df[key].iloc[-1]\n",
    "    # Show\n",
    "    print(\n",
    "        f\"[{i}] Mean: {mean:.2f} Â± {sd:.2f} Range: {min_:.2f} to {max_:.2f}, Depth start: {start}, end: {end}\"\n",
    "    )\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.plot(df[key], label=key)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    # Datasets that defy column value norms\n",
    "    #     if (min_ <= 0) or (max_ <= 0):\n",
    "    #         print(\"\\tMin or Max non-positive.\")\n",
    "    #         val_exception[url] = (mean, sd, min_, max_, start, end)\n",
    "    if value_near_zero(start) or value_near_zero(end):\n",
    "        print(\"\\tStart or End near zero.\")\n",
    "        val_exception[url] = (mean, sd, min_, max_, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5758133",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75247f02",
   "metadata": {},
   "source": [
    "### 2.2 Bathy depth\n",
    "**Observations:**\n",
    "- ***Bathy depth*** values in ALL datasets are positive.\n",
    "- It is reasonable to assume that bathymetry depth refers to the distance from mean sea level to the ocean floor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to find\n",
    "key = \"bathy depth\"\n",
    "\n",
    "val_exception = {}\n",
    "for i, file in enumerate(column_examples[key]):\n",
    "    df = pd.read_csv(os.path.join(dirname, file), low_memory=False)\n",
    "    url_column = find_url_column(df)\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    # Extract info\n",
    "    mean = df[key].mean()\n",
    "    sd = df[key].std()\n",
    "    min_ = df[key].min()\n",
    "    max_ = df[key].max()\n",
    "    url = get_dataset_url(file)\n",
    "    # Show\n",
    "    print(f\"[{i}] Mean: {mean:.2f} Â± {sd:.2f} Range: {min_:.2f} to {max_:.2f}\")\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.plot(df[key], label=key)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    if (min_ < 0) or (max_ < 0):\n",
    "        print(\"\\tDoes not satisfy column value norms.\")\n",
    "        val_exception[url] = (mean, sd, min_, max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10508935",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e13eb",
   "metadata": {},
   "source": [
    "### 2.3 Depth top & depth bot\n",
    "**Observations:**\n",
    "- Common sense dictates that ***depth top*** and ***depth bot*** should mean the depth of the top and bottom of the camera vehicle.\n",
    "- With this assumtion we would expect the difference between top and bot depth to be constant.\n",
    "- In cases where the difference varies this is likely due to the rotation of the camera vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836128a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to find\n",
    "keys = [\"depth top\", \"depth bot\"]\n",
    "# Depth bot & depth top\n",
    "\n",
    "for i, file in enumerate(column_examples[keys[0]]):\n",
    "    df = pd.read_csv(os.path.join(dirname, file), low_memory=False)\n",
    "    url_column = find_url_column(df)\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    for key in keys:\n",
    "        # Extract info\n",
    "        mean = df[key].mean()\n",
    "        sd = df[key].std()\n",
    "        min_ = df[key].min()\n",
    "        max_ = df[key].max()\n",
    "        url = get_dataset_url(file)\n",
    "        # Show\n",
    "        print(\n",
    "            f\"[{i}] '{key}' Mean: {mean:.2f} Â± {sd:.2f} Range: {min_:.2f} to {max_:.2f}\"\n",
    "        )\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for key in keys:\n",
    "        plt.plot(df[key], label=key)\n",
    "    plt.plot(abs(df[\"depth top\"] - df[\"depth bot\"]), label=\"diff\", linestyle=\":\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e54609",
   "metadata": {},
   "source": [
    "## 3. Explore relation between depth columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(column_examples[\"depth\"]))\n",
    "print(len(column_examples[\"depth water\"]))\n",
    "print(len(column_examples[\"bathy depth\"]))\n",
    "print(len(column_examples[\"bathy depth_2\"]))\n",
    "print(len(column_examples[\"bathy_depth\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bbde6",
   "metadata": {},
   "source": [
    "## 3.1 Datasets with `depth`, `depth water` and `bathy depth` columns\n",
    "When depth co-occurs with bathy depth and depth water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_set = set(column_examples[\"depth\"])\n",
    "depth_water_set = set(column_examples[\"depth water\"])\n",
    "bathy_set = set(column_examples[\"bathy depth\"])\n",
    "intersect = depth_set.intersection(depth_water_set).intersection(bathy_set)\n",
    "\n",
    "print(\"depth_set :\", len(depth_set))\n",
    "print(\"depth_water_set :\", len(depth_water_set))\n",
    "print(\"bathy_set :\", len(bathy_set))\n",
    "print(\"# of files with all:\", len(intersect))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c40022",
   "metadata": {},
   "source": [
    "## 3.2 Datasets with `depth` and `bathy depth` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3744ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_set = set(column_examples[\"depth\"])\n",
    "bathy_set = set(column_examples[\"bathy depth\"])\n",
    "intersect = depth_set.intersection(bathy_set)\n",
    "\n",
    "print(\"depth_set :\", len(depth_set))\n",
    "print(\"bathy_set :\", len(bathy_set))\n",
    "print(\"# of files with both:\", len(intersect))\n",
    "\n",
    "keys = [\"depth\", \"bathy depth\"]\n",
    "if len(intersect) > 0:\n",
    "    for file in intersect:\n",
    "        df = pd.read_csv(os.path.join(dirname, file), low_memory=False)\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        for key in keys:\n",
    "            # Extract info\n",
    "            mean = df[key].mean()\n",
    "            sd = df[key].std()\n",
    "            min_ = df[key].min()\n",
    "            max_ = df[key].max()\n",
    "            url = get_dataset_url(file)\n",
    "            # Show\n",
    "            print(\n",
    "                f\"[{i}] '{key}' Mean: {mean:.2f} Â± {sd:.2f} Range: {min_:.2f} to {max_:.2f}\"\n",
    "            )\n",
    "        # Plot\n",
    "        plt.figure(figsize=(16, 4))\n",
    "        for key in keys:\n",
    "            plt.plot(-df[key], label=key.capitalize())\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d508bd94",
   "metadata": {},
   "source": [
    "**NOTE:** for datasets with both depth and bathy depth, the bathy depth seems to be the depth of the sea floor (relative to mean sea level) and depth seems to be the depth of the camera vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec486ea",
   "metadata": {},
   "source": [
    "## 3.3 Datasets with `depth water` and `bathy depth` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcafbf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_water_set = set(column_examples[\"depth water\"])\n",
    "bathy_set = set(column_examples[\"bathy depth\"])\n",
    "intersect = depth_water_set.intersection(bathy_set)\n",
    "\n",
    "print(\"depth_water_set :\", len(depth_water_set))\n",
    "print(\"bathy_set :\", len(bathy_set))\n",
    "print(\"# of files with both:\", len(intersect))\n",
    "\n",
    "keys = [\"depth water\", \"bathy depth\"]\n",
    "if len(intersect) > 0:\n",
    "    for file in intersect:\n",
    "        df = pd.read_csv(os.path.join(dirname, file), low_memory=False)\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        for key in keys:\n",
    "            # Extract info\n",
    "            mean = df[key].mean()\n",
    "            sd = df[key].std()\n",
    "            min_ = df[key].min()\n",
    "            max_ = df[key].max()\n",
    "            url = get_dataset_url(file)\n",
    "            # Show\n",
    "            print(\n",
    "                f\"[{i}] '{key}' Mean: {mean:.2f} Â± {sd:.2f} Range: {min_:.2f} to {max_:.2f}\"\n",
    "            )\n",
    "        # Plot\n",
    "        plt.figure(figsize=(16, 4))\n",
    "        for key in keys:\n",
    "            plt.plot(-df[key], label=key.capitalize())\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf2703",
   "metadata": {},
   "source": [
    "**NOTE:**<br>\n",
    "- Depth water is the altitude of the craft below mean sea level.\n",
    "- Bathy depth is the bathymetry depth or the depth of the sea floor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38ffb7",
   "metadata": {},
   "source": [
    "## 3.4 Datasets with two `bathy depth` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e281a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_set = set(column_examples[\"bathy depth\"])\n",
    "bathy_set2 = set(column_examples[\"bathy depth_2\"])\n",
    "intersect = bathy_set.intersection(bathy_set2)\n",
    "\n",
    "print(\"bathy_set :\", len(bathy_set))\n",
    "print(\"bathy_set2 :\", len(bathy_set2))\n",
    "print(\"# of files with both:\", len(intersect))\n",
    "\n",
    "keys = [\"bathy depth\", \"bathy depth_2\"]\n",
    "if len(intersect) > 0:\n",
    "    for file in intersect:\n",
    "        df = pd.read_csv(os.path.join(dirname, file))\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        print(df.doi.iloc[0])\n",
    "        for key in keys:\n",
    "            # Extract info\n",
    "            mean = df[key].mean()\n",
    "            sd = df[key].std()\n",
    "            min_ = df[key].min()\n",
    "            max_ = df[key].max()\n",
    "            url = get_dataset_url(file)\n",
    "            # Show\n",
    "            print(\n",
    "                f\"[{i}] '{key}' Mean: {mean:.2f} Â± {sd:.2f} Range: {min_:.2f} to {max_:.2f}\"\n",
    "            )\n",
    "        # Plot\n",
    "        plt.figure(figsize=(16, 4))\n",
    "        for key in keys:\n",
    "            plt.plot(df[key], label=key.capitalize())\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5a3b8",
   "metadata": {},
   "source": [
    "**NOTE:** Upon checking the dataset webpages we see that the two bathy depth columns correspond to the original collection and recollection sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabda82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (ws)",
   "language": "python",
   "name": "ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
